<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[大话一致性哈希算法]]></title>
    <url>%2F2019%2F08%2F08%2F%E5%A4%A7%E8%AF%9D%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[应用场景 假如我们有六个节点的redis集群（A，B，C，D，E，F），我们要缓存几十万，几百万甚至几千万的优惠券数据。为了提升我的性能，提高我的响应速度，我肯定是希望这些优惠券能够均匀的分布在这六台redis机器上。当我们需要这些数据的时候，能够直接去对应的机器上查找。 原始做法看到上面这个问题，我们肯定是能够想到先对这些优惠券hash，然后再对6（因为是6个机器）取模，最后得出的结果就是该优惠券信息存放的机器的片号： hash(优惠券id) mod 6 这样的话，不管是新增还是查询优惠券信息，都实现通过这个公式计算一下，然后决定落到哪个节点上。 但是，但是，但是，如果突然间我们的产品比较给力，深受客户的喜欢，用我们的优惠券的越来越多啦，6台机器已经满足不了我们的业务啦，那我们怎么办？那肯定是要扩充节点呗，一狠心一咬牙（毕竟得花钱买机器嘛），决定把6台机器扩充到10台机器，那么各位客官，此时问题就来啦，扩充完机器后，公式就变成hash(优惠券id) mod 10,新增的优惠券信息还好，但是原有的信息咋办呢，比如我的优惠券id为1的优惠券存放在A机器上，这是一个未扩充机器时候的优惠券，但是扩充机器之后，我算出的机器片号是C，我去C机器上根本找不到该优惠券啊。What‘s Fuck!!!!!!! 一致性hash算法那么此时，就要用到一致性hash算法啦。 其实，一致性hash算法也是用的hash取模的方法，但是取模的不再是对机器数取模，而是对2^32取模。 首先，我们得把2^32想象成一个闭环，我们把所有的机器放在这个环上，公式如下： hash(机器IP) mod 2^32 然后再看一下优惠券数据如何在环上体现的？还是对数据进行hash，然后对2^32取模： hash(优惠券id) mod 2^32 好了，现在服务器和优惠券数据都已经在环上啦，那这些优惠券数据到底被缓存到哪台服务器上了呢？ 沿着优惠券数据顺时针走到的第一台服务器，那么该数据就会缓存到该服务器上。 如上图所示： 优惠券1,优惠券2，优惠券3都被缓存到B服务器上 机器C上没有缓存任何优惠券数据 优惠券4,优惠券5被缓存到D机器上 优惠券6,优惠券7被缓存到E机器上 优惠券8,优惠券9,优惠券10被缓存到F机器上 机器A没有缓存任何优惠券数据 hash环的偏移上述的方法真的能够完全解决一致性hash的问题吗? 当数据量大的时候，有可能会造成大部分的数据都被缓存到一台服务器上，而其他的服务器缓存到的数据很少，就会造成环的不平衡。那么当缓存大量数据的服务器宕机之后，那就会造成大量的缓存丢失，然后就会穿库，造成雪崩。 这种情况我们怎么办呢？一致性hash算法中使用了”虚拟节点“解决了这个问题。 虚拟节点就是当发生hash环偏移的时候，我们就造出来一些虚拟的节点，让这些数据尽可能均匀的分布到每台机器上。 虚拟节点是实际节点在hash环上复制品，一个实际节点可以对应多个虚拟节点。 ​]]></content>
      <tags>
        <tag>一致性hash</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高并发下数据库与缓存的双写一致性]]></title>
    <url>%2F2019%2F07%2F30%2F%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8E%E7%BC%93%E5%AD%98%E7%9A%84%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7%2F</url>
    <content type="text"><![CDATA[今天和建志兄讨论了一下高并发下的数据库与缓存的双写一致性的可行性方案，又参考了一下网上大神们的文章，颇有感触，所以mark一下~~~ 一说起缓存一致性，那就是涉及到缓存的更新策略，而缓存的更新策略无外乎三种： 1.先更新数据库，再更新缓存 2.先删除缓存，再更新数据库 3.先更新数据库，再删除缓存 PS：从理论上来说，给缓存设置过期时间，可以保证数据的最终一致性。 先更新数据库，再更新缓存 其实这个策略是被大多数人鄙视的，因为这会导致脏数据的出现，比如下面这个案例： 线程A做更新操作 线程B做更新操作 线程A更新了数据库 线程B更新了数据库 线程B更新了缓存 线程A更新了缓存 本来线程A更新数据早于线程B，所以此时最终的数据应该是B更新的数据，但是线程B先更新了缓存，然后线程A再更新缓存，这就导致了脏数据。 还有一点就是根据业务场景的不同，有些写入缓存的数据是经过几个表的数据计算叠加在一起的，每次更新数据库后，还要重新连表查询计算再放入缓存，对性能造成很大的浪费。 先删除缓存，再更新数据库有以下场景： 线程A做更新操作 线程B做查询操作 线程A进行写操作，删除缓存 线程B查询数据，发现缓存数据不存在 线程B穿库查询旧值并写入数据到缓存 线程A把新数据更新到数据库中，此时数据库中的数据是最终的，但缓存的数据还是旧值 这种情况，可以先给缓存设置过期时间，但是如果缓存数据一致没过期，那读取到的数据就一直是脏数据。 所以，采用延时双删策略： 先删除缓存 再更新数据库 休眠1秒（根据业务场景定），再次删除缓存 我们既然是高并发环境，那我们真实的生产环境下的数据库就肯定不是单机的，肯定是集群的，并且是读写分离的，读写分离的数据库架构下又怎么办呢？ 线程A更新数据，删除缓存 线程A将数据更新到主库中 线程B查询缓存，未找到 线程B穿从库查询，此时，还未进行主从同步，所以查询的值是旧值 线程B将旧值写入缓存 数据库完成主从同步，从库变为新值 也是采用双删延时策略，睡眠时间为主从同步的延时时间加上业务处理的时间. 但是如果第二次删除缓存失败了，那咋办？ 先更新数据库，再删除缓存这种情况就不会有数据不一致问题吗？ 线程A做查询操作 线程B做更新操作 缓存刚好失效 线程A查询数据库，得到一个旧值 线程B更新新值到数据库 线程B删除缓存 请求A将查询到的旧值更新到缓存中 如果法伤以上情况，就有可能会出现脏数据 但是出现这种情况的概率又有多大呢？ 要想出现上述情况，步骤4先于步骤5，那就要求写数据的耗时要比读数据的耗时更短，这是基本不会出现的，因为读操作比较快，耗资源少，这就是读写分离的目的。 但是如果我们非得抬杠呢，那怎么解决呢？ 那就设置过期时间，或者采用双删延时策略。 但是这个和上一个策略都有一个问题，那就是如果第二次删除缓存失败咋办？ 重试机制针对二次删除缓存失败的问题，我们可以提供一个保障的重试机制： 放入mq进行重试 监听mysql binlog日志]]></content>
      <tags>
        <tag>redis</tag>
        <tag>数据一致性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker学习-redis-cluster搭建三主三从集群]]></title>
    <url>%2F2019%2F05%2F27%2FDocker%E5%AD%A6%E4%B9%A0-redis-cluster%E6%90%AD%E5%BB%BA%E4%B8%89%E4%B8%BB%E4%B8%89%E4%BB%8E%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[docker搭建redis集群可是费了我好大的力气，刚开始的方法就是不能把六个节点加入到一个集群中，一直在那儿waiting for the cluster to join，本来以为是集群总线的端口没有开放的原因，可并非如此。最终经过几天的斗争，终于搞定啦，使用redis-cluster搭建了一个三主三从的集群，mark一下… 拉取redis和ruby镜像 docker pull redis:3.0.7 docker pull ruby 拉取完之后，下载redis:3.0.7版本的redis.conf文件，修改cluster配置： 创建一个文件夹，专门存放docker搭建redis集群所需的文件： mkdir redis-cluster 然后把上述修改的redis.conf文件拷贝到redis-cluster文件夹下 创建redis-server镜像在redis-cluster文件夹下创建一个Dockerfile文件，内容如下： 1234FROM redis:5.0.3EXPOSE 6379ADD redis.conf /redis.confENTRYPOINT [ "redis-server", "/redis.conf" ] 然后开始执行build： docker build -t “你自己镜像的名字” . docker images 看一下： 创建好镜像之后，为了使不同的集群端口之间相互不冲突，自己创建一个network: 1docker network create --subnet=172.19.0.0/16 redis-network 创建完之后，docker network ls 查看一下： 然后运行三主三从六个节点： 1234567891011docker run -d --net redis-network --ip 172.19.0.91 -p 6001:6379 --name redis-6001 zhjs/redis-serverdocker run -d --net redis-network --ip 172.19.0.92 -p 6002:6379 --name redis-6002 zhjs/redis-serverdocker run -d --net redis-network --ip 172.19.0.93 -p 6003:6379 --name redis-6003 zhjs/redis-serverdocker run -d --net redis-network --ip 172.19.0.94 -p 6004:6379 --name redis-6004 zhjs/redis-serverdocker run -d --net redis-network --ip 172.19.0.95 -p 6005:6379 --name redis-6005 zhjs/redis-serverdocker run -d --net redis-network --ip 172.19.0.96 -p 6006:6379 --name redis-6006 zhjs/redis-server 创建完上述六个节点容器之后，docker ps： 然后在redis-cluster目录下创建一个文件夹ruby，里面放redis-trib.rb,下载对应redis版本的redis-trib.rb， 然后创建一个文件Dockerfile,里面的内容如下： 123FROM rubyADD redis-trib.rb /redis-trib.rb 在ruby目录下执行: docker build -t “你自己的image名字” . 如下图所示： 创建好了之后，在跟刚才运行的redis节点相同的network环境下执行： 1docker run --net=redis-network --ip 172.19.0.100 --name ruby-zhjs -i -d ruby-redis 然后进入ruby容器： 1docker exec --it ruby-zhjs bash 进入容器之后，执行: 1gem install redis --version 3.0.7 然后把刚才创建的六个节点添加到一个集群中去： 1./redis-trib.rb create --replicas 1 172.19.0.91:6379 172.19.0.92:6379 172.19.0.93:6379 172.19.0.94:6379 172.19.0.95:6379 172.19.0.96:6379 到此为止，你就搭建好了一个三主三从的redis集群，现在就去验证一下吧： 随便进入一个节点容器，然后执行: redis-cli -c cluster nodes info replication 从上图发现，这六个节点已经搭建成了一个集群。设置几个值试一下吧：]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker学习-Dockerfile制作MySQL镜像]]></title>
    <url>%2F2019%2F05%2F23%2FDocker%E5%AD%A6%E4%B9%A0-Dockerfile%E5%88%B6%E4%BD%9CMySQL%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[制作一个属于自己的MySQL镜像 首先在本机创建my.cnf文件，文件内容如下： 123456789101112131415161718192021#输入以下内容[mysqld]##################基础设置##################pid-file = /var/run/mysqld/mysqld.pidsocket = /var/run/mysqld/mysqld.sockdatadir = /var/lib/mysql#log-error = /var/log/mysql/error.logsql_mode=STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTIONdefault-time-zone=&apos;+08:00&apos;#数据库默认字符集,主流字符集支持一些特殊表情符号（特殊表情符占用4个字节）character-set-server = utf8mb4#数据库字符集对应一些排序等规则，注意要和character-set-server对应collation-server = utf8mb4_general_ci#设置client连接mysql时的字符集,防止乱码init_connect=‘SET NAMES utf8mb4‘#是否对sql语句大小写敏感，1表示不敏感lower_case_table_names = 1 然后新建Dockerfile文件，文件内容如下： 1234FROM mysql:5.7COPY my.cnf /etc/mysql/EXPOSE 3306CMD [&quot;mysqld&quot;] 编译: docker build -t zhjs/mysql:5.7 . 注意： 最后面那个.一定要加 时间有点长，等待ing… 终于完成啦，docker images一下： 运行容器： 在Dockerfile文件所在的目录下创建三个文件夹 mkdir conf logs data data目录将映射为mysql容器配置的数据文件存放路径 logs目录将映射为mysql容器的日志目录 conf目录里的配置文件将映射为mysql容器的配置文件 创建这三个目录可以让我们更方便的查看容器里的MySQL的日志和数据信息 1docker run -d -p 3305:3306 -v $PWD/conf:/etc/mysql/conf.d -v $PWD/logs:/logs -v $PWD/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 zhjs/mysql:5.7 启动之后,docker ps 一下： 最后用navicat连接查看： 好了，你的MySQL镜像制作完成啦，上传到你的docker hub上去吧。 PS: 本机文件目录结构：]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker部署ES+Kibana]]></title>
    <url>%2F2019%2F05%2F23%2FDocker%E5%AD%A6%E4%B9%A0-%E9%83%A8%E7%BD%B2ES-Kibana%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[最近学习docker，瞬间感觉docker的强大，被docker的魅力深深折服，所以打算把我本机所包含的服务全部部署在docker容器中，本篇是第一篇ES+Kibana的学习，mark一下~ ES镜像直接使用docker Hub上的镜像，还是比较强大的，话不多说，先上命令： docker pull docker.elastic.co/elasticsearch/elasticsearch-oss:6.3.0 然后在本机运行开发模式下的es： docker run -d –name es-zhjs -p 9000:9200 -p 9100:9300 -e ES_JAVA_OPTS=”-Xms512m -Xmx512m” -e “http.host=0.0.0.0” -e “transport.host=127.0.0.1” docker.elastic.co/elasticsearch/elasticsearch-oss:6.3.0 注意： 因为我本机是安装es的，端口是9200和9300，所以为了防止端口冲突，我就重新开启了端口9000和9100 启动之后，docker ps一下，可以看到： 在浏览器上输入127.0.0.1:9000，就可以看到如下所示啦： Kibana镜像拉取官方的镜像： docker pull docker.elastic.co/kibana/kibana-oss:6.3.0 运行Kibana： docker run –name kibana-zhjs -p 5600:5601 -e “ELASTICSEARCH_URL=http://172.17.0.4:9200&quot; -d docker.elastic.co/kibana/kibana-oss:6.3.0 注意： 此处的172.17.0.4是我的es镜像的主机ip，刚开始我连接127.0.0.1:9000的时候，打开Kibana界面，发现连接不上es，所以我就直接写了es镜像的ip，具体查看方法如下： 进入es镜像： docker exec -it es-zhjs bash 查看主机ip: cat /etc/hosts 启动之后，就可以看到容器啦。 在浏览器上输入127.0.0.1:5600，就可以看到如下界面啦： 至此，docker部署es+Kibana就结束啦，开始你的docker之旅吧~~~]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker学习-docker部署搭建我的第一个springboot项目]]></title>
    <url>%2F2019%2F05%2F16%2FDocker%E5%AD%A6%E4%B9%A0-docker%E9%83%A8%E7%BD%B2%E6%90%AD%E5%BB%BA%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AAspringboot%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[前言 现如今，Docker越来越火，我们的项目也使用docker部署啦，所以我闲暇时间就学习了下docker，自己时间了一下。 搭建Docker​ 网上有很多搭建Docker的教程，我找到一个写的还不错的，各种版本的都有 各位看官请看 传送门 搭建springboot项目首先先搭建一个springboot项目 然后在src/main目录下创建一个docker文件夹，里面创建一个名字叫Dockerfile的文件。 注意：文件名称一定要叫Dockerfile，大小写都不能改变。 文件目录如下： Dockerfile文件内容： 1234FROM java:8VOLUME /tmpADD spring-boot-es-0.0.1-SNAPSHOT.jar springboot-es.jarENTRYPOINT [&quot;java&quot;,&quot;-Djava.security.egd=file:/dev/./urandom&quot;,&quot;-jar&quot;,&quot;/springboot-es.jar&quot;] From jdk版本 VOLUME 编译后的docker镜像放在/var/lib/tmp目录下 ADD 拷贝jar包到容器中 注意：spring-boot-es-0.0.1-SNAPSHOT.jar名称必须和pom文件中配置的artifactId+version一致 ENTRYPOINT 执行jar文件 配置pom文件： 12345678910111213141516171819202122232425262728&lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;docker.image.prefix&gt;zhjs&lt;/docker.image.prefix&gt;&lt;/properties&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt; &lt;version&gt;0.4.13&lt;/version&gt; &lt;configuration&gt; &lt;imageName&gt;$&#123;docker.image.prefix&#125;/$&#123;project.artifactId&#125;&lt;/imageName&gt; &lt;dockerDirectory&gt;src/main/docker&lt;/dockerDirectory&gt; &lt;resources&gt; &lt;resource&gt; &lt;targetPath&gt;/&lt;/targetPath&gt; &lt;directory&gt;$&#123;project.build.directory&#125;&lt;/directory&gt; &lt;include&gt;$&#123;project.build.finalName&#125;.jar&lt;/include&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 配置完之后，开始build: 1mvn package docker:build build之后，可以看一下本地有哪些镜像： docker images 然后启动服务： docker run -d -p 8080:8080 zhjs/spring-boot-es -d 代表在后台运行 -p 标识宿主机与docker服务的端口映射，注意谁前谁后：【宿主端口：docker内服务端口】 zhjs/spring-boot-es就是启动镜像的名称，当然了使用IMAGE ID 也是可以的 启动好啦，看一下服务是否启动成功了吧 docker ps 最后在浏览器中，访问项目（localhost:8080）就可以啦 github项目：传送门]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[互联网寒冬下的面试总结]]></title>
    <url>%2F2019%2F04%2F13%2F%E4%BA%92%E8%81%94%E7%BD%91%E5%AF%92%E5%86%AC%E4%B8%8B%E7%9A%84%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[​ 在这金三银四的季节中，我也加入了面试大军，经过这段时间的面试，对这个互联网寒冬的到来也是深有体会。偶尔也会在地铁上遇到好多和我一样，拿着简历，背着书包，不知是赶着去面试还是面试回来，但是能看得出来表情中透露出的无奈。不同于以往，现在每家面试，都会有十好几位候选人，而且要求都特别的高，因为在面试官眼中，我不招你，还会有比你更优秀的人，但是对于你来说，你可能会很长一段时间才能找到一个适合自己的公司。也有身边的同事和朋友说，好多人都是面试好几个月才能找到一个心仪的公司。 ​ 面试了这么多家，面试经验也积攒了不少，在这记录一下，有些面试题可能只是记住大概，也有些问题已经忘记啦，下面的答案是LZ自己的回答或者是面后在网上找的答案，如有不同意见，敬请提出！ 某支付大厂M​ 接到M的面试还是比较意外的，其实感觉自己有点傻，不应该第一个就面试M，毕竟好长时间都没有面试过啦，一点感觉都没有啦，应该先面试几个小厂找找感觉，然后再面试大厂，这样过的几率可能会比较大。 ​ 首先是在线笔试题，晚上八点半登录M厂的在线笔试网站做了笔试题，一共三道题，比较简单，基础。 1.统计给定文件中给定字符串的出现次数 123456789101112131415161718192021222324252627282930313233343536373839404142/** * @param filename 文件名 * @param word 字符串 * @return 字符串在文件中出现的次数 */ public static int countWord(String filename, String word) &#123; FileReader fr = null; BufferedReader br = null; int count = 0; StringBuilder sb = new StringBuilder(); try &#123; fr = new FileReader(new File(filename)); br = new BufferedReader(fr); String keyWord = null; while((keyWord = br.readLine()) != null)&#123; sb.append(keyWord); &#125; int currIndex = 0; while(currIndex &gt;= 0)&#123; currIndex = sb.toString().indexOf(word,currIndex + 1); count ++; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; if(null != br)&#123; try &#123; br.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if(null != fr)&#123; try &#123; fr.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; return count; &#125; 2.10个线程去完成第一题 123456789101112131415161718192021222324252627public static int countWordByThreads(String filename, String word) throws FileNotFoundException &#123; AtomicInteger count = new AtomicInteger(0); FileReader fr = new FileReader(filename); try(BufferedReader br = new BufferedReader(fr))&#123; StringBuffer sb = new StringBuffer(); while (true)&#123; String line = br.readLine(); if (line == null)&#123; break; &#125; sb.append(line); &#125; String result = sb.toString(); int index = 0; while (true)&#123; index = result.indexOf(word,index+1); if (index&gt;0)&#123; count.getAndIncrement(); &#125;else &#123; break; &#125; &#125; &#125;catch (IOException e)&#123; e.printStackTrace(); &#125; return count.get(); &#125; 3.取出两个List的差集 123456789101112/** * @param list1 * @param list2 * @return 差集元素 */ public &lt;T extends Comparable&lt;T&gt;&gt; List&lt;T&gt; pickDiff(List&lt;T&gt; list1, List&lt;T&gt; list2)&#123; List&lt;T&gt; result = list1.stream().filter(item-&gt; !list2.contains(item)).collect(Collectors.toList()); List&lt;T&gt; result2 = list2.stream().filter(item-&gt; !list1.contains(item)).collect(Collectors.toList()); result.addAll(result2); return result; &#125; 4.通过反射对象获取指定字段的值 12345678910111213141516171819202122/** * 通过反射取对象指定字段(属性)的值 * @param target 目标对象 * @param fieldName 字段的名字 * @throws 如果取不到对象指定字段的值则抛出异常 * @return 字段的值 */ public static Object getValue(Object target, String fieldName) &#123; Class clazz = target.getClass(); Object result = null; try &#123; Field field = clazz.getDeclaredField(fieldName); field.setAccessible(true); result = field.get(target); &#125; catch (NoSuchFieldException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; return result; &#125; 答完笔试题之后，面试官接着就打过来电话，让我给他讲了一下我做的这几道题的解法，然后就约了第二天下午的电话一面。 自我介绍介绍其中一个项目 1.多线程参数 ​ ThreadPoolExecutor executor = new ThreadPoolExecutor(核心线程数量，最大线程数量，线程活跃时间 ，线程活跃时间单位，阻塞队列，线程工厂，拒绝策略); 2.假如一个线程核心线程数是10，最大线程数是20，当加入1个任务后，线程池是怎么工作的? ​ 加入一个任务后，如果线程池的线程数还没有达到10个，就分配一个线程执行任务；当大于10个小于20个，加入到阻塞队列中；当大于20个，拒绝策略打回。 3.redis的持久化 rdb和aof rdb是默认的持久化策略，原理是将redis内存中的数据库记录定时dump到磁盘上，实际操作就是fork一个子进程，先将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储。 aof是redis的操作日志以追加的方式写入文件。 4.redis的基本类型 String，List，Hash，Set，Zset 5.Zset的底层实现 跳表。具体实现可以查看网上的讲解，比较全，还是比较简单的。 6.给一个双向链表，查找倒数第K个节点？时间复杂度和空间复杂度是多少？ 两个指针，一个从头结点出发，一个从尾结点出发，两个结点的距离在K-1的时候就查找到倒数第K个节点。 7.一个列表，里面是对象People，根据name相等，去重？ 重写equals方法。 8.给定一个数组，int类型，如何保证把里面所有的移到最后面，然后其他的参数保持相对不变？ 这道题不太记得具体的题目啦，但是至今没明白面试官的意思~~~ 9.spring的声明周期? Bean的建立， 由BeanFactory读取Bean定义文件，并生成各个实例Setter注入，执行Bean的属性依赖注入BeanNameAware的setBeanName(), 如果实现该接口，则执行其setBeanName方法BeanFactoryAware的setBeanFactory()，如果实现该接口，则执行其setBeanFactory方法BeanPostProcessor的processBeforeInitialization()，如果有关联的processor，则在Bean初始化之前都会执行这个实例的processBeforeInitialization()方法InitializingBean的afterPropertiesSet()，如果实现了该接口，则执行其afterPropertiesSet()方法Bean定义文件中定义init-methodBeanPostProcessors的processAfterInitialization()，如果有关联的processor，则在Bean初始化之前都会执行这个实例的processAfterInitialization()方法DisposableBean的destroy()，在容器关闭时，如果Bean类实现了该接口，则执行它的destroy()方法Bean定义文件中定义destroy-method，在容器关闭时，可以在Bean定义文件中使用“destory-method”定义的方法 10.spring的IOC工作原理？ 就是将bean的创建和依赖的管理注入交由spring容器控制。 某在线旅行金融事业部Q1.redis的主从同步机制： ​ 配置好slave服务器连接的master后，slave会建立和master的连接，然后发送sync命令。无论是第一次同步建立的连接还是连接断开后的重新连接，master都会启动一个后台进程，将数据库快照保存到文件中.同时master主进程会开始收集新的写命令并缓存起来。当后台进程完成写文件后，master就将快照文件发送给slave，slave将文件保存到磁盘上，然后加载到内存将数据库快照恢复到slave上。slave完成快照文件的恢复后，master就会把缓存的命令都转发给slave，slave更新内存数据库。后续master收到的写命令都会通过开始建立的连接发送给slave。从master到slave的同步数据的命令和从 client到master发送的命令使用相同的协议格式。当master和slave的连接断开时，slave可以自动重新建立连接。如果master同时收到多个slave发来的同步连接命令，只会使用启动一个进程来写数据库镜像，然后发送给所有slave。 2.redis一致性hash算法： 将用户和redis节点的hash值对应到一个32位的环形数据结构上，环形结构首尾封闭，用户通过hash算法来定位在环形结构上，redis节点也通过hash算法来定位到环形结构上，此时的命中问题就变成了，用户节点通过顺时针旋转，在旋转的过程中若碰到redis节点，就在该节点上读取数据，若此时在环形结构上增加新的redis节点，由于是顺时针寻找对应的redis节点，所以用户此时的redis命中率还是很高的，不会因为增加了一台redis节点就导致大量的用户命中失败的情况出现。 3.跳表的实现： 网上资料比较全 4.spring是如何解决两个bean的循环依赖的 spring只能解决单例模式下的循环依赖，把scope设置为singleton 5.MySQL数据库索引B+Tree的实现： ​ https://www.cnblogs.com/tiancai/p/9024351.html 6.MyISAM和InnoDB的区别 MyISAM InnoDB 版本 MySQL5.5之前的默认存储引擎 MySQL5.6之后的默认存储引擎 事物 不支持 也不支持ACID 支持 锁 表锁 行锁 外键 不支持 支持 存储 文件中 表空间 隔离等级 无 所有 性能 高性能读取，保存了表的行数，使用count时不需要全表扫描 使用count需要全表扫描 7.事物： 四大隔离级别： READ UNCOMMITTED(未提交读)。在RU的隔离级别下，事务A对数据做的修改，即使没有提交，对于事务B来说也是可见的，这种问题叫脏读。这是隔离程度较低的一种隔离级别，在实际运用中会引起很多问题，因此一般不常用。 READ COMMITTED(提交读)。在RC的隔离级别下，不会出现脏读的问题。事务A对数据做的修改，提交之后会对事务B可见，举例，事务B开启时读到数据1，接下来事务A开启，把这个数据改成2，提交，B再次读取这个数据，会读到最新的数据2。在RC的隔离级别下，会出现不可重复读的问题。这个隔离级别是许多数据库的默认隔离级别。 REPEATABLEREAD(可重复读)。在RR的隔离级别下，不会出现不可重复读的问题。事务A对数据做的修改，提交之后，对于先于事务A开启的事务是不可见的。举例，事务B开启时读到数据1，接下来事务A开启，把这个数据改成2，提交，B再次读取这个数据，仍然只能读到1。在RR的隔离级别下，会出现幻读的问题。幻读的意思是，当某个事务在读取某个范围内的值的时候，另外一个事务在这个范围内插入了新记录，那么之前的事务再次读取这个范围的值，会读取到新插入的数据。Mysql默认的隔离级别是RR，然而mysql的innoDB引擎间隙锁成功解决了幻读的问题。 SERIALIZABLE(可串行化)。可串行化是最高的隔离级别。这种隔离级别强制要求所有事物串行执行，在这种隔离级别下，读取的每行数据都加锁，会导致大量的锁征用问题，性能最差。 8.SQL优化： 查询条件减少使用函数，避免全表扫描 减少不必要的表连接 有些数据操作的业务逻辑可以放到应用层进行实现 可以使用with as 尽量避免使用游标，因为游标的效率较差 不要把SQL语句写得太复杂 不能循环执行查询 用 exists 代替 in 表关联关系不要太纠结 查询多用索引列取查，用charindex或者like[0-9]来代替%% inner关联的表可以先查出来，再去关联leftjoin的表 可以进行表关联数据拆分，即先查出核心数据，再通过核心数据查其他数据，这样会快得多 参考SQL执行顺序进行优化 表关联时取别名，也能提高效率 使用视图，给视图建立索引进行优化 使用数据仓库的形式，建立单独的表存储数据，根据时间戳定期更新数据。将多表关联的数据集中抽取存入一张表中，查询时单表查询，提高了查询效率 对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引 应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如： select id from t where num is null 可以在num上设置默认值0，确保表中num列没有null值，然后这样查询： select id from t where num=0 应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描 当使用like时，后面的字段，后面加%可以使用索引 比如：select * from student where name like ‘%张%’;———未使用索引 ​ select * from student where name like ‘张%’;———使用索引 ​ select * from student where name like ‘%张’;———未使用索引 可以通过执行计划看到 9.volatile在实际项目中的使用场景 其他1.双亲委派 2.类加载顺序 ​ 父类静态代码块-&gt;子类静态代码块-&gt;父类构造方法-&gt;子类构造方法 3.synchronized是如何做到同步的 4.nginx的LB算法 轮询（默认） weight(按权重) ip_hash Url_hash Fair:可以根据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间 来分配请求，响应时间短的优先分配 5.concurrentHashMap的实现6.乐观锁和悲观锁7.分布式事物 TCC 8.查找一个单向链表的最中间那个节点： 两个指针A和B，都指向头节点，A每次走一个节点，B每次走两个节点，当B走到尾节点时，A所在的节点即为最中间节点。 9.下面的打印不是同时打印的： 1234567891011121314151617181920212223public class Demo2 &#123; public static void main(String[] args) &#123; System.out.println(Test2.a);//JD 无需初始化实例// System.out.println(Test2.b);//OKJD 初始化// System.out.println(Test2.c);//OKJD 初始化// System.out.println(Test2.d);//OK1 //如果是由static和final修饰的String，不需要对Test2初始化就可以得到值 //但是如果a换成Integer a = 1;则需要先初始化才可以，此时得到的值是OK1 //必须确保static和final是一个编译期常量 //1.J ok,2.J ok,3.ok J,4.1 ok &#125;&#125;class Test2&#123; public static final String a = "JD"; public static final String b = new String("JD"); public static String c = "JD"; public static final Integer d = 1; static &#123; System.out.print("OK"); &#125;&#125;]]></content>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper重复注册]]></title>
    <url>%2F2019%2F01%2F09%2Fzookeeper%E9%87%8D%E5%A4%8D%E6%B3%A8%E5%86%8C%2F</url>
    <content type="text"><![CDATA[昨天在部署项目的时候，突然间发现其中一个项目的一个服务（同一ip，同一端口，同一路径）注册到zk上三遍，只是节点名称不同而已。 在网上查了好多资料，有说是hosts的问题，也有说是服务器和客户端的版本不一致导致的，这些我都验证啦，没有什么问题，最后发现是项目中的注解的问题。这个项目是2年前的项目，当时做这个项目的人可能没搞懂spring的@Service注解和dubbo的@Service的区别，导致了引错包。 本来我们大多数使用的都是spring的@Service注解，但是这个引用成了dubbo的@Service注解，其实这也无所谓，主要是在配置文件中，多注册了一次。 上图标记1处就是和dubbo的@Service注解配合使用，使用dubbo:annotation直接可以将所有扫描包下的添加了dubbo的@Service注解的服务注册到zk上；关键是他还配置了标记2处的代码，导致又往zk注册上一遍服务，再加上本身的@Service注解的注册，所以就导致了三次注入zk。 具体有关spring的@Service和dubbo的@Service注解的区别，网上有很多的资料，可以自行查阅。]]></content>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMq学习笔记---原理解析及概念]]></title>
    <url>%2F2018%2F12%2F18%2FRocketMq%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90%E5%8F%8A%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[一.前言LZ之前是一直在用rabbitmq的，虽说知道rocketmq的存在，但是一直没有去研究。但直到今年rocketmq发布的4.3版本中，支持了分布式事物，瞬间引起了LZ的注意，看了一下rmq的分布式事物的设计，感觉还不错，于是乎就想着去学习一下rocketmq。 二. RocketMq 的物理部署 三. RocketMq的几个重要的概念1.broker: 消息中转角色，存储消息，转发消息。其实就是我们所说的Mq。Broker接收生产者的消息，存储以及为消费者拉取消息做准备。 2.name Server: 接收broker的请求注册信息；保存活跃的broker列表；下图是namesrv在整个rmq中的结构： 3.producer: 消息生产者，负责产生消息，一般由业务系统负责产生消息。 4.Consumer: 消息消费者，负责消费消息，一般是后台系统负责异步消费。5.Push Consumer: Consumer 的一种，应用通常吐 Consumer 对象注册一个 Listener 接口，一旦收到消息，Consumer 对象立刻回调 Listener 接口方法。6.Pull Consumer: Consumer 的一种，应用通常主动调用 Consumer 的拉消息方法从 Broker 拉消息，主动权由应用控制。7.Producer Group : 一类 Producer 的集合名称。8.Consumer Group：一类 Consumer 的集合名称。 9.广播消费：一条消息被多个 Consumer 消费，即使这些 Consumer 属于同一个 Consumer Group，消息也会被 ConsumerGroup 中的每个 Consumer 都消费一次，广播消费中的 Consumer Group 概念可以认为在消息划分方面无意义。 10.集群消费：一个 Consumer Group 中的 Consumer 实例平均分摊消费消息。例如某个 Topic 有 9 条消息，其中一个Consumer Group 有 3 个实例（可能是 3 个进程，或者 3 台机器），那么每个实例只消费其中的 3 条消息。在 CORBA Notification 规范中，无此消费方式。在 JMS 规范中，JMS point-to-point model 与之类似，但是 RocketMQ 的集群消费功能大等于 PTP 模型。因为 RocketMQ 单个 Consumer Group 内的消费者类似于 PTP，但是一个 Topic/Queue 可以被多个 ConsumerGroup 消费。 11.顺序消息：消费消息的顺序要同接收消息的顺序一致，在 RocketMQ 中，主要指的是局部顺序，即一类消息为满足顺序性，必须 Producer 单线程顺序发送，且发送到同一个队列，这样 Consumer 就可以按照 Producer 发送的顺序去消费消息。12.普通顺序消息：顺序消息的一种，正常情况下可以保证完全的顺序消息，但是一旦发生通信异常，Broker 重启，由于队列总数发生发化，哈希取模后定位的队列会发化，产生短暂的消息顺序不一致。如果业务能容忍在集群异常情况（如某个 Broker 宕机或者重启）下，消息短暂的乱序，使用普通顺序方式比较合适。13严格顺序消息：顺序消息的一种，无论正常异常情况都能保证顺序，但是牺牲了分布式 Failover 特性，即 Broker 集群中只要有一台机器不可用，则整个集群都不可用，服务可用性大大降低。如果服务器部署为同步双写模式，此缺陷可通过备机自动切换为主避免，不过仍然会存在几分钟的服务不可用。目前已知的应用只有数据库 binlog 同步强依赖严格顺序消息，其他应用绝大部分都可以容忍短暂乱序，推荐使用普通的顺序消息。14.Message Queue：在 RocketMQ 中，所有消息队列都是持久化，长度无限的数据结构，所谓长度无限是指队列中的每个存储单元都是定长，访问其中的存储单元使用 Offset 来访问，offset 为 java long 类型，64 位，理论上在 100年内不会溢出，所以认为是长度无限，另外队列中只保存最近几天的数据，之前的数据会按照过期时间来删除。也可以认为 Message Queue 是一个长度无限的数组，offset 就是下标。 四. Broker的几种搭建形式1.单个Master模式 : 这种方式风险较大，一旦 Broker 重启或者宕机时，会导致整个服务不可用，不建议线上环境使用 2.多个Master模式： 一个集群无 Slave，全是 Master，例如 2 个 Master 戒者 3 个 Master 优点：配置简单，单个 Master 宕机或重启维护对应用无影响，在磁盘配置为 RAID10 时，即使机器宕机不可恢复情况下，由于 RAID10 磁盘非常可靠，消息也不会丢（异步刷盘丢失少量消息，同步刷盘一条会丢）。性能最高。 缺点：单台机器宕机期间，这台机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会受到影响。 3.多Msater多slave模式，异步复制 ： 每个 Master 配置一个 Slave，有多对 Master-Slave，HA 采用异步复制方式，主备有短暂消息延迟，毫秒级。 优点：即使磁盘损坏，消息丢失的非常少，且消息实时性不会受影响，因为 Master 宕机后，消费者仍然可以从 Slave 消费，此过程对应用透明。不需要人工干预。性能同多 Master 模式几乎一样。 缺点：Master 宕机，磁盘损坏情况，会丢失少量消息 4.多Master多slave模式，同步双写 ： 每个 Master 配置一个 Slave，有多对 Master-Slave，HA 采用同步双写方式，主备都写成功，向应用返回成功。 优点：数据不服务都无单点，Master 宕机情况下，消息无延迟，服务可用性与数据可用性都非常高 缺点：性能比异步复制模式略低，大约低 10%左右，发送单个消息的 RT 会略高。目前主宕机后，备机不能能自动切换为主机，后续会支持自动切换功能。]]></content>
      <tags>
        <tag>RocketMq</tag>
        <tag>中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单例模式下的静态成员加载的顺序]]></title>
    <url>%2F2018%2F12%2F06%2F%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84%E9%9D%99%E6%80%81%E6%88%90%E5%91%98%E5%8A%A0%E8%BD%BD%E7%9A%84%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[各位客官，请看代码： 1234567891011121314151617public class Singleton &#123; private static Singleton instance = new Singleton(); private static int x = 0; private static int y; Singleton()&#123; x++; y++; &#125; private static Singleton getInstance()&#123; return instance; &#125; public static void main(String[] args)&#123; Singleton s = Singleton.getInstance(); System.out.println(s.x); System.out.println(s.y); &#125;&#125; 这段代码如何输出呢？ 如果没有仔细看的话，肯定会以为是输出1，1。 但是，但是，但是，实际的结果是0，1。这是为什么呢? 这与Java的初始化顺序有关，按照以上的代码，完整是顺序应该是： 第一步，JVM调用main方法，因为main方法时static的，所以JVM会先构造所有的静态成员。指针初始化为null，数值为0。此时，x=0;y=0 第二步，按照自上而下的顺序，分别进行静态成员的赋值操作。先进行的是Singleton instance = new Singleton(); 此时，x=1;y=1 第三步，执行int x = 0; 此时，x=0;y=1 第四步，执行int y。没有赋值操作。此时，x=0;y=1 所以最后的结果是0，1. 如果你把private static Singleton instance = new Singleton()放在下面又是不一样的结果： 1234567891011121314151617public class Singleton &#123; private static int x = 0; private static int y; private static Singleton instance = new Singleton(); Singleton()&#123; x++; y++; &#125; private static Singleton getInstance()&#123; return instance; &#125; public static void main(String[] args)&#123; Singleton s = Singleton.getInstance(); System.out.println(s.x); System.out.println(s.y); &#125;&#125; 此时，输出的结果就是1，1啦。 PS： java静态成员和非静态成员的类加载机制也不一样： clinit .java =&gt; 编译 =&gt; .class =&gt; clinit =&gt; 类构造器 类构造器 将静态变量（初始化）和静态语句块收敛（将一系列操作集合起来执行）到clinit收敛顺序：父类静态变量初始化，父类静态语句块，子类静态变量初始化，子类静态语句块， 父类是接口，则不调用父类的clinit 若一个类没有static 语句块，或者static 变量赋值，则可以没有clinit clinit 类加载时期执行 init .java =&gt; 编译 =&gt; .class =&gt; init =&gt; 实例构造器 实例构造器 将变量（初始化），语句块，父类构造器，收敛到init收敛顺序（非静态）：父类变量初始化，父类语句块，父类构造函数，子类变量初始化，子类语句块，子类构造器 init 对象实例化时期执行 对象实例化执行的整个过程 父类静态变量初始化，父类静态语句块，子类静态变量初始化，子类静态语句块，父类变量初始化，父类语句块，父类构造函数，子类变量初始化，子类语句块，子类构造器。]]></content>
      <tags>
        <tag>JVM</tag>
        <tag>单例模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Log4j转为Log4j2]]></title>
    <url>%2F2018%2F11%2F28%2FLog4j%E8%BD%AC%E4%B8%BALog4j2%2F</url>
    <content type="text"><![CDATA[一. 前言​ 之前项目是用的log4j+slf4j，但在高并发的情况下，日志的输出影响到了接口的响应时间，所以打算把项目改为log4j2+slf4j的形式，并使用log4j2的全异步日志形式，最大限度的提高日志的输出效率。 二. log4j，slf4j，log4j2的介绍log4j直接在pom文件中引用log4j-1.2.17.jar依赖就可以啦 slf4jslf4j不是一个具体的日志实现方案，是为Java程序提供统一的日志输出接口。就像JDBC一样，只是一种规则。所以slf4j是不能单独工作的，必须搭配其他的日志实现方案 log4j+slf4j如果我们在系统中需要使用slf4j和log4j来进行日志输出的话，我们需要引入下面的jar包：log4j核心jar包：log4j-1.2.17.jarslf4j核心jar包：slf4j-api-1.6.4.jar slf4j与log4j的桥接包：slf4j-log4j12-1.6.1.jar，这个包的作用就是使用slf4j的api，但是底层实现是基于log4j slf4j+log4j2如果我们在系统中需要使用slf4j和log4j2来进行日志输出的话，我们需要引入下面的jar包：log4j2核心jar包：log4j-api-2.7.jar和log4j-core-2.7.jarslf4j核心jar包：slf4j-api-1.6.4.jar slf4j与log4j2的桥接包：log4j-slf4j-impl-2.7.jar，这个包的作用就是使用slf4j的api，但是底层实现是基于log4j2 log4j21.关于配置文件的名称以及在项目中的存放位置 log4j 2.x版本不再支持像1.x中的.properties后缀的文件配置方式，2.x版本配置文件后缀名只能为”.xml”,”.json”或者”.jsn”. 系统选择配置文件的优先级(从先到后)如下： (1).classpath下的名为log4j2-test.json 或者log4j2-test.jsn的文件. (2).classpath下的名为log4j2-test.xml的文件. (3).classpath下名为log4j2.json 或者log4j2.jsn的文件. (4).classpath下名为log4j2.xml的文件. 2.缺省默认配置文件12345678910111213&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;Configuration status="WARN"&gt; &lt;Appenders&gt; &lt;Console name="Console" target="SYSTEM_OUT"&gt; &lt;PatternLayout pattern="%d&#123;HH:mm:ss.SSS&#125; [%t] %-5level %logger&#123;36&#125; - %msg%n"/&gt; &lt;/Console&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Root level="error"&gt; &lt;AppenderRef ref="Console"/&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 3.配置文件节点解析(1).根节点Configuration有两个属性:status和monitorinterval,有两个子节点:Appenders和Loggers(表明可以定义多个Appender和Logger). status用来指定log4j本身的打印日志的级别. monitorinterval用于指定log4j自动重新配置的监测间隔时间，单位是s,最小是5s. (2).Appenders节点，常见的有三种子节点:Console、RollingFile、File. Console节点用来定义输出到控制台的Appender. name:指定Appender的名字. target:SYSTEM_OUT 或 SYSTEM_ERR,一般只设置默认:SYSTEM_OUT. PatternLayout:输出格式，不设置默认为:%m%n. File节点用来定义输出到指定位置的文件的Appender. name:指定Appender的名字. fileName:指定输出日志的目的文件带全路径的文件名. PatternLayout:输出格式，不设置默认为:%m%n. RollingFile节点用来定义超过指定大小自动删除旧的创建新的的Appender. name:指定Appender的名字. fileName:指定输出日志的目的文件带全路径的文件名. PatternLayout:输出格式，不设置默认为:%m%n. filePattern:指定新建日志文件的名称格式. Policies:指定滚动日志的策略，就是什么时候进行新建日志文件输出日志. TimeBasedTriggeringPolicy:Policies子节点，基于时间的滚动策略，interval属性用来指定多久滚动一次，默认是1 hour。modulate=true用来调整时间：比如现在是早上3am，interval是4，那么第一次滚动是在4am，接着是8am，12am…而不是7am. SizeBasedTriggeringPolicy:Policies子节点，基于指定文件大小的滚动策略，size属性用来定义每个日志文件的大小. DefaultRolloverStrategy:用来指定同一个文件夹下最多有几个日志文件时开始删除最旧的，创建新的(通过max属性)。 (3).Loggers节点，常见的有两种:Root和Logger. Root节点用来指定项目的根日志，如果没有单独指定Logger，那么就会默认使用该Root日志输出 level:日志输出级别，共有8个级别，按照从低到高为：All &lt; Trace &lt; Debug &lt; Info &lt; Warn &lt; Error &lt; Fatal &lt; OFF. AppenderRef：Root的子节点，用来指定该日志输出到哪个Appender. Logger节点用来单独指定日志的形式，比如要为指定包下的class指定不同的日志级别等。 level:日志输出级别，共有8个级别，按照从低到高为：All &lt; Trace &lt; Debug &lt; Info &lt; Warn &lt; Error &lt; Fatal &lt; OFF. name:用来指定该Logger所适用的类或者类所在的包全路径,继承自Root节点. AppenderRef：Logger的子节点，用来指定该日志输出到哪个Appender,如果没有指定，就会默认继承自Root.如果指定了，那么会在指定的这个Appender和Root的Appender中都会输出，此时我们可以设置Logger的additivity=”false”只在自定义的Appender中进行输出。 (4).关于日志level. 共有8个级别，按照从低到高为：All &lt; Trace &lt; Debug &lt; Info &lt; Warn &lt; Error &lt; Fatal &lt; OFF. All:最低等级的，用于打开所有日志记录. Trace:是追踪，就是程序推进以下，你就可以写个trace输出，所以trace应该会特别多，不过没关系，我们可以设置最低日志级别不让他输出. Debug:指出细粒度信息事件对调试应用程序是非常有帮助的. Info:消息在粗粒度级别上突出强调应用程序的运行过程. Warn:输出警告及warn以下级别的日志. Error:输出错误信息日志. Fatal:输出每个严重的错误事件将会导致应用程序的退出的日志. OFF:最高等级的，用于关闭所有日志记录. 程序会打印高于或等于所设置级别的日志，设置的日志等级越高，打印出来的日志就越少。 4.比较完整的log4j2.xml配置模板12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;!--日志级别以及优先级排序: OFF &gt; FATAL &gt; ERROR &gt; WARN &gt; INFO &gt; DEBUG &gt; TRACE &gt; ALL --&gt; &lt;!--Configuration后面的status，这个用于设置log4j2自身内部的信息输出，可以不设置，当设置成trace时，你会看到log4j2内部各种详细输出--&gt; &lt;!--monitorInterval：Log4j能够自动检测修改配置 文件和重新配置本身，设置间隔秒数--&gt; &lt;configuration status="WARN" monitorInterval="30"&gt; &lt;!--先定义所有的appender--&gt; &lt;appenders&gt; &lt;!--这个输出控制台的配置--&gt; &lt;console name="Console" target="SYSTEM_OUT"&gt; &lt;!--输出日志的格式--&gt; &lt;PatternLayout pattern="[%d&#123;HH:mm:ss:SSS&#125;] [%p] - %l - %m%n"/&gt; &lt;/console&gt; &lt;!--文件会打印出所有信息，这个log每次运行程序会自动清空，由append属性决定，这个也挺有用的，适合临时测试用--&gt; &lt;File name="log" fileName="log/test.log" append="false"&gt; &lt;PatternLayout pattern="%d&#123;HH:mm:ss.SSS&#125; %-5level %class&#123;36&#125; %L %M - %msg%xEx%n"/&gt; &lt;/File&gt; &lt;!-- 这个会打印出所有的info及以下级别的信息，每次大小超过size，则这size大小的日志会自动存入按年份-月份建立的文件夹下面并进行压缩，作为存档--&gt; &lt;RollingFile name="RollingFileInfo" fileName="$&#123;sys:user.home&#125;/logs/info.log" filePattern="$&#123;sys:user.home&#125;/logs/$$&#123;date:yyyy-MM&#125;/info-%d&#123;yyyy-MM-dd&#125;-%i.log"&gt; &lt;!--控制台只输出level及以上级别的信息（onMatch），其他的直接拒绝（onMismatch）--&gt; &lt;ThresholdFilter level="info" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;PatternLayout pattern="[%d&#123;HH:mm:ss:SSS&#125;] [%p] - %l - %m%n"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size="100 MB"/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;RollingFile name="RollingFileWarn" fileName="$&#123;sys:user.home&#125;/logs/warn.log" filePattern="$&#123;sys:user.home&#125;/logs/$$&#123;date:yyyy-MM&#125;/warn-%d&#123;yyyy-MM-dd&#125;-%i.log"&gt; &lt;ThresholdFilter level="warn" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;PatternLayout pattern="[%d&#123;HH:mm:ss:SSS&#125;] [%p] - %l - %m%n"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size="100 MB"/&gt; &lt;/Policies&gt; &lt;!-- DefaultRolloverStrategy属性如不设置，则默认为最多同一文件夹下7个文件，这里设置了20 --&gt; &lt;DefaultRolloverStrategy max="20"/&gt; &lt;/RollingFile&gt; &lt;RollingFile name="RollingFileError" fileName="$&#123;sys:user.home&#125;/logs/error.log" filePattern="$&#123;sys:user.home&#125;/logs/$$&#123;date:yyyy-MM&#125;/error-%d&#123;yyyy-MM-dd&#125;-%i.log"&gt; &lt;ThresholdFilter level="error" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;PatternLayout pattern="[%d&#123;HH:mm:ss:SSS&#125;] [%p] - %l - %m%n"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size="100 MB"/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;/appenders&gt; &lt;!--然后定义logger，只有定义了logger并引入的appender，appender才会生效--&gt; &lt;loggers&gt; &lt;!--过滤掉spring和mybatis的一些无用的DEBUG信息--&gt; &lt;logger name="org.springframework" level="INFO"&gt;&lt;/logger&gt; &lt;logger name="org.mybatis" level="INFO"&gt;&lt;/logger&gt; &lt;root level="all"&gt; &lt;appender-ref ref="Console"/&gt; &lt;appender-ref ref="RollingFileInfo"/&gt; &lt;appender-ref ref="RollingFileWarn"/&gt; &lt;appender-ref ref="RollingFileError"/&gt; &lt;/root&gt; &lt;/loggers&gt; &lt;/configuration&gt; 三. 搭建步骤3.1 pom文件 12345678910111213141516171819202122232425262728293031323334 &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.13&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;jcl-over-slf4j&lt;/artifactId&gt; &lt;version&gt;1.7.13&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt;&lt;!--核心log4j2jar包--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;version&gt;2.4.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.4.1&lt;/version&gt;&lt;/dependency&gt;&lt;!--用于与slf4j保持桥接--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt; &lt;version&gt;2.4.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- log4j2异步日志需要加载disruptor-3.0.0.jar或者更高的版本 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.lmax&lt;/groupId&gt; &lt;artifactId&gt;disruptor&lt;/artifactId&gt; &lt;version&gt;3.4.0&lt;/version&gt;&lt;/dependency&gt; 3.2 log4j2.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788&lt;?xml version="1.0" encoding= "UTF-8"?&gt;&lt;Configuration status="off" monitorInterval="1800"&gt; &lt;Properties&gt; &lt;property name="projectName"&gt;activity&lt;/property&gt; &lt;property name="logPattern"&gt;[%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;] [%t] %-5level %logger&#123;36&#125;-%msg%n&lt;/property&gt; &lt;property name="logPath"&gt;/app/logs/sell/sellActServer&lt;/property&gt; &lt;property name="fileName"&gt;$&#123;logPath&#125;/$&#123;projectName&#125;/info.log&lt;/property&gt; &lt;property name="errorFileName"&gt;$&#123;logPath&#125;/$&#123;projectName&#125;/error.log&lt;/property&gt; &lt;property name="warnFileName"&gt;$&#123;logPath&#125;/$&#123;projectName&#125;/warn.log&lt;/property&gt; &lt;property name="rollingFilePattern"&gt;$&#123;logPath&#125;/$&#123;projectName&#125;/info.log.%d&#123;yyyy-MM-dd-HH&#125;&lt;/property&gt; &lt;property name="errorRollingFilePattern"&gt;$&#123;logPath&#125;/$&#123;projectName&#125;/error.log.%d&#123;yyyy-MM-dd-HH&#125;&lt;/property&gt; &lt;property name="warnRollingFilePattern"&gt;$&#123;logPath&#125;/$&#123;projectName&#125;/warn.log.%d&#123;yyyy-MM-dd-HH&#125;&lt;/property&gt; &lt;property name="everyFileSize"&gt;3 MB&lt;/property&gt; &lt;property name="maxFiles"&gt;50&lt;/property&gt; &lt;/Properties&gt; &lt;Appenders&gt; &lt;!-- 所有日志 --&gt; &lt;RollingFile name="RollingFile" fileName="$&#123;fileName&#125;" filePattern="$&#123;rollingFilePattern&#125;"&gt; &lt;PatternLayout pattern="$&#123;logPattern&#125;" /&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy /&gt; &lt;!-- &lt;SizeBasedTriggeringPolicy size="$&#123;everyFileSize&#125;" /&gt; --&gt; &lt;/Policies&gt; &lt;!-- &lt;DefaultRolloverStrategy max="$&#123;maxFiles&#125;" /&gt; --&gt; &lt;/RollingFile&gt; &lt;!-- warn 日志 --&gt; &lt;RollingFile name="WarnRollingFile" fileName="$&#123;warnFileName&#125;" filePattern="$&#123;warnRollingFilePattern&#125;"&gt; &lt;PatternLayout pattern="$&#123;logPattern&#125;"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy /&gt; &lt;/Policies&gt; &lt;Filters&gt; &lt;ThresholdFilter level="ERROR" onMatch="DENY" onMismatch="NEUTRAL"/&gt; &lt;ThresholdFilter level="WARN" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;/Filters&gt; &lt;/RollingFile&gt; &lt;!-- error日志 --&gt; &lt;RollingFile name="ErrorRollingFile" fileName="$&#123;errorFileName&#125;" filePattern="$&#123;errorRollingFilePattern&#125;"&gt; &lt;PatternLayout pattern="$&#123;logPattern&#125;" /&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy /&gt; &lt;!-- &lt;SizeBasedTriggeringPolicy size="$&#123;everyFileSize&#125;" /&gt; --&gt; &lt;/Policies&gt; &lt;!-- &lt;DefaultRolloverStrategy max="$&#123;maxFiles&#125;" /&gt; --&gt; &lt;Filters&gt; &lt;ThresholdFilter level="error" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;/Filters&gt; &lt;/RollingFile&gt; &lt;Console name="Console" target="SYSTEM_OUT" ignoreExceptions="false"&gt; &lt;PatternLayout pattern="$&#123;logPattern&#125;" /&gt; &lt;/Console&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;logger name="com.zhjs" level="debug"/&gt; &lt;logger name="com.zhjs.sell.activity.service.util" level="info"/&gt; &lt;logger name="com.alibaba.dubbo" level="info"/&gt; &lt;logger name="com.zhjs.sell.common.web.filter.RequestLogFilter" level="debug"/&gt; &lt;logger name="com.zhjs.db.mybatis.memcached" level="info"/&gt; &lt;logger name="com.zhjs.db.mybatis.memcached.MemcachedCache" level="debug"/&gt; &lt;Root level="info"&gt; &lt;AppenderRef ref="Console" /&gt; &lt;AppenderRef ref="RollingFile" /&gt; &lt;AppenderRef ref="WarnRollingFile" /&gt; &lt;AppenderRef ref="ErrorRollingFile" /&gt; &lt;/Root&gt; &lt;/Loggers&gt;&lt;/Configuration&gt; 3.3 全异步日志第一种方式： JVM启动参数（boot.ini）加上“-DLog4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector” 第二种方式： classpath中添加文件“log4j2.component.properties”，文件增加以下内容：“Log4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector” 第三种方式： log4j2.xml配置文件中使用AsyncRoot/AsyncLogger替代Root/Logger 第四种方式： 在主程序的开头加一句： 1System.setProperty("Log4jContextSelector", "org.apache.logging.log4j.core.async.AsyncLoggerContextSelector");]]></content>
      <tags>
        <tag>Log4j2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM原理]]></title>
    <url>%2F2018%2F11%2F26%2FJVM%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[一.Java虚拟机的声明周期Java虚拟机的生命周期 一个运行中的Java虚拟机有着一个清晰的任务：执行Java程序。程序开始执行时他才运行，程序结束时他就停止。你在同一台机器上运行三个程序，就会有三个运行中的Java虚拟机。 Java虚拟机总是开始于一个main()方法，这个方法必须是公有、返回void、直接受一个字符串数组。在程序执行时，你必须给Java虚拟机指明这个包换main()方法的类名。 Main()方法是程序的起点，他被执行的线程初始化为程序的初始线程。程序中其他的线程都由他来启动。Java中的线程分为两种：守护线程 （daemon）和普通线程（non-daemon）。守护线程是Java虚拟机自己使用的线程，比如负责垃圾收集的线程就是一个守护线程。当然，你也可 以把自己的程序设置为守护线程。包含Main()方法的初始线程不是守护线程。 只要Java虚拟机中还有普通的线程在执行，Java虚拟机就不会停止。如果有足够的权限，你可以调用exit()方法终止程序。 二.Java虚拟机的体系结构​ 在Java虚拟机的规范中定义了一系列的子系统、内存区域、数据类型和使用指南。这些组件构成了Java虚拟机的内部结构，他们不仅仅为Java虚拟机的实现提供了清晰的内部结构，更是严格规定了Java虚拟机实现的外部行为。​ 每一个Java虚拟机都由一个类加载器子系统（class loader subsystem），负责加载程序中的类型（类和接口），并赋予唯一的名字。每一个Java虚拟机都有一个执行引擎（execution engine）负责执行被加载类中包含的指令。​ 程序的执行需要一定的内存空间，如字节码、被加载类的其他额外信息、程序中的对象、方法的参数、返回值、本地变量、处理的中间变量等等。Java虚拟机将 这些信息统统保存在数据区（data areas）中。虽然每个Java虚拟机的实现中都包含数据区，但是Java虚拟机规范对数据区的规定却非常的抽象。许多结构上的细节部分都留给了 Java虚拟机实现者自己发挥。不同Java虚拟机实现上的内存结构千差万别。一部分实现可能占用很多内存，而其他以下可能只占用很少的内存；一些实现可 能会使用虚拟内存，而其他的则不使用。这种比较精炼的Java虚拟机内存规约，可以使得Java虚拟机可以在广泛的平台上被实现。​ 数据区中的一部分是整个程序共有，其他部分被单独的线程控制。每一个Java虚拟机都包含方法区（method area）和堆（heap），他们都被整个程序共享。Java虚拟机加载并解析一个类以后，将从类文件中解析出来的信息保存与方法区中。程序执行时创建的 对象都保存在堆中。​ 当一个线程被创建时，会被分配只属于他自己的PC寄存器“pc register”（程序计数器）和Java堆栈（Java stack）。当线程不掉用本地方法时，PC寄存器中保存线程执行的下一条指令。Java堆栈保存了一个线程调用方法时的状态，包括本地变量、调用方法的 参数、返回值、处理的中间变量。调用本地方法时的状态保存在本地方法堆栈中（native method stacks），可能再寄存器或者其他非平台独立的内存中。​ Java堆栈有堆栈块（stack frames (or frames)）组成。堆栈块包含Java方法调用的状态。当一个线程调用一个方法时，Java虚拟机会将一个新的块压到Java堆栈中，当这个方法运行结束时，Java虚拟机会将对应的块弹出并抛弃。​ Java虚拟机不使用寄存器保存计算的中间结果，而是用Java堆栈在存放中间结果。这是的Java虚拟机的指令更紧凑，也更容易在一个没有寄存器的设备上实现Java虚拟机。 三.类加载器子系统Java虚拟机中的类加载器分为两种：原始类加载器（primordial class loader）和类加载器对象（class loader objects）。原始类加载器是Java虚拟机实现的一部分，类加载器对象是运行中的程序的一部分。不同类加载器加载的类被不同的命名空间所分割。​ 类加载器调用了许多Java虚拟机中其他的部分和java.lang包中的很多类。比如，类加载对象就是java.lang.ClassLoader子类 的实例，ClassLoader类中的方法可以访问虚拟机中的类加载机制；每一个被Java虚拟机加载的类都会被表示为一个 java.lang.Class类的实例。像其他对象一样，类加载器对象和Class对象都保存在堆中，被加载的信息被保存在方法区中。​ 1、加载、连接、初始化（Loading, Linking and Initialization）类加载子系统不仅仅负责定位并加载类文件，他按照以下严格的步骤作了很多其他的事情：（具体的信息参见第七章的“类的生命周期”）​ 1）、加载：寻找并导入指定类型（类和接口）的二进制信息​ 2）、连接：进行验证、准备和解析​ ①验证：确保导入类型的正确性​ ②准备：为类型分配内存并初始化为默认值​ ③解析：将字符引用解析为直接饮用​ 3）、初始化：调用Java代码，初始化类变量为合适的值​ 2、原始类加载器（The Primordial Class Loader）​ 每个Java虚拟机都必须实现一个原始类加载器，他能够加载那些遵守类文件格式并且被信任的类。但是，Java虚拟机的规范并没有定义如何加载类，这由 Java虚拟机实现者自己决定。对于给定类型名的类型，原始莱加载器必须找到那个类型名加“.class”的文件并加载入虚拟机中。​ 3、类加载器对象​ 虽然类加载器对象是Java程序的一部分，但是ClassLoader类中的三个方法可以访问Java虚拟机中的类加载子系统。​ 1）、protected final Class defineClass(…)：使用这个方法可以出入一个字节数组，定义一个新的类型。​ 2）、protected Class findSystemClass(String name)：加载指定的类，如果已经加载，就直接返回。​ 3）、protected final void resolveClass(Class c)：defineClass()方法只是加载一个类，这个方法负责后续的动态连接和初始化。​ 具体的信息，参见第八章“连接模型”（ The Linking Model）。​ 4、命名空间​ 当多个类加载器加载了同一个类时，为了保证他们名字的唯一性，需要在类名前加上加载该类的类加载器的标识。具体的信息，参见第八章“连接模型”（ The Linking Model）。 四.方法区在Java虚拟机中，被加载类型的信息都保存在方法区中。这写信息在内存中的组织形式由虚拟机的实现者定义，比如，虚拟机工作在一个“little- endian”的处理器上，他就可以将信息保存为“little-endian”格式的，虽然在Java类文件中他们是以“big-endian”格式保 存的。设计者可以用最适合并地机器的表示格式来存储数据，以保证程序能够以最快的速度执行。但是，在一个只有很小内存的设备上，虚拟机的实现者就不会占用 很大的内存。​ 程序中的所有线程共享一个方法区，所以访问方法区信息的方法必须是线程安全的。如果你有两个线程都去加载一个叫Lava的类，那只能由一个线程被容许去加载这个类，另一个必须等待。​ 在程序运行时，方法区的大小是可变的，程序在运行时可以扩展。有些Java虚拟机的实现也可以通过参数也订制方法区的初始大小，最小值和最大值。​ 方法区也可以被垃圾收集。因为程序中的内由类加载器动态加载，所有类可能变成没有被引用（unreferenced）的状态。当类变成这种状态时，他就可 能被垃圾收集掉。没有加载的类包括两种状态，一种是真正的没有加载，另一个种是“unreferenced”的状态。详细信息参见第七章的类的生命周期 （The Lifetime of a Class）。​ 1、类型信息（Type Information）​ 每一个被加载的类型，在Java虚拟机中都会在方法区中保存如下信息：​ 1）、类型的全名（The fully qualified name of the type）​ 2）、类型的父类型的全名（除非没有父类型，或者弗雷形式java.lang.Object）（The fully qualified name of the typeís direct superclass）​ 3）、给类型是一个类还是接口（class or an interface）（Whether or not the type is a class ）​ 4）、类型的修饰符（public，private，protected，static，final，volatile，transient等）（The typeís modifiers）​ 5）、所有父接口全名的列表（An ordered list of the fully qualified names of any direct superinterfaces）​ 类型全名保存的数据结构由虚拟机实现者定义。除此之外，Java虚拟机还要为每个类型保存如下信息：​ 1）、类型的常量池（The constant pool for the type）​ 2）、类型字段的信息（Field information）​ 3）、类型方法的信息（Method information）​ 4）、所有的静态类变量（非常量）信息（All class (static) variables declared in the type, except constants）​ 5）、一个指向类加载器的引用（A reference to class ClassLoader）​ 6）、一个指向Class类的引用（A reference to class Class） 1）、类型的常量池（The constant pool for the type） 常量池中保存中所有类型是用的有序的常量集合，包含直接常量（literals）如字符串、整数、浮点数的常量，和对类型、字段、方法的符号引用。常量池 中每一个保存的常量都有一个索引，就像数组中的字段一样。因为常量池中保存中所有类型使用到的类型、字段、方法的字符引用，所以它也是动态连接的主要对 象。详细信息参见第六章“The Java Class File”。 2）、类型字段的信息（Field information） 字段名、字段类型、字段的修饰符（public，private，protected，static，final，volatile，transient等）、字段在类中定义的顺序。 3）、类型方法的信息（Method information） 方法名、方法的返回值类型（或者是void）、方法参数的个数、类型和他们的顺序、字段的修饰符（public，private，protected，static，final，volatile，transient等）、方法在类中定义的顺序 如果不是抽象和本地本法还需要保存 方法的字节码、方法的操作数堆栈的大小和本地变量区的大小（稍候有详细信息）、异常列表（详细信息参见第十七章“Exceptions”。） 4）、类（静态）变量（Class Variables） 类变量被所有类的实例共享，即使不通过类的实例也可以访问。这些变量绑定在类上（而不是类的实例上），所以他们是类的逻辑数据的一部分。在Java虚拟机使用这个类之前就需要为类变量（non-final）分配内存 常量（final）的处理方式于这种类变量（non-final）不一样。每一个类型在用到一个常量的时候，都会复制一份到自己的常量池中。常量也像类变 量一样保存在方法区中，只不过他保存在常量池中。（可能是，类变量被所有实例共享，而常量池是每个实例独有的）。Non-final类变量保存为定义他的 类型数据（data for the type that declares them）的一部分，而final常量保存为使用他的类型数据（data for any type that uses them）的一部分。详情参见第六章“The Java Class FileThe Java Class File” 5）、指向类加载器的引用（A reference to class ClassLoader） 每一个被Java虚拟机加载的类型，虚拟机必须保存这个类型是否由原始类加载器或者类加载器加载。那些被类加载器加载的类型必须保存一个指向类加载器的引 用。当类加载器动态连接时，会使用这条信息。当一个类引用另一个类时，虚拟机必须保存那个被引用的类型是被同一个类加载器加载的，这也是虚拟机维护不同命 名空间的过程。详情参见第八章“The Linking Model” 6）、指向Class类的引用（A reference to class Class） Java虚拟机为每一个加载的类型创建一个java.lang.Class类的实例。你也可以通过Class类的方法： public static Class forName(String className)来查找或者加载一个类，并取得相应的Class类的实例。通过这个Class类的实例，我们可以访问Java虚拟机方法区中的信息。具体参照Class类的JavaDoc。​ 2、方法列表（Method Tables）​ 为了更有效的访问所有保存在方法区中的数据，这些数据的存储结构必须经过仔细的设计。所有方法区中，除了保存了上边的那些原始信息外，还有一个为了加快存 取速度而设计的数据结构，比如方法列表。每一个被加载的非抽象类，Java虚拟机都会为他们产生一个方法列表，这个列表中保存了这个类可能调用的所有实例 方法的引用，报错那些父类中调用的方法。详情参见第八章“The Linking Model”。 五.堆12345678910111213141516当Java程序创建一个类的实例或者数组时，都在堆中为新的对象分配内存。虚拟机中只有一个堆，所有的线程都共享他。 1、垃圾收集（Garbage Collection） 垃圾收集是释放没有被引用的对象的主要方法。它也可能会为了减少堆的碎片，而移动对象。在Java虚拟机的规范中没有严格定义垃圾收集，只是定义一个Java虚拟机的实现必须通过某种方式管理自己的堆。详情参见第九章“Garbage Collection”。 2、对象存储结构（Object Representation） Java虚拟机的规范中没有定义对象怎样在堆中存储。每一个对象主要存储的是他的类和父类中定义的对象变量。对于给定的对象的引用，虚拟机必须嫩耨很快的 定位到这个对象的数据。另为，必须提供一种通过对象的引用方法对象数据的方法，比如方法区中的对象的引用，所以一个对象保存的数据中往往含有一个某种形式 指向方法区的指针。 一个可能的堆的设计是将堆分为两个部分：引用池和对象池。一个对象的引用就是指向引用池的本地指针。每一个引用池中的条目都包含两个部分：指向对象池中对 象数据的指针和方法区中对象类数据的指针。这种设计能够方便Java虚拟机堆碎片的整理。当虚拟机在对象池中移动一个对象的时候，只需要修改对应引用池中 的指针地址。但是每次访问对象的数据都需要处理两次指针。下图演示了这种堆的设计。在第九章的“垃圾收集”中的HeapOfFish Applet演示了这种设计。 另一种堆的设计是：一个对象的引用就是一个指向一堆数据和指向相应对象的偏移指针。这种设计方便了对象的访问，可是对象的移动要变的异常复杂。下图演示了这种设计 当程序试图将一个对象转换为另一种类型时，虚拟机需要判断这种转换是否是这个对象的类型，或者是他的父类型。当程序适用instanceof语句的时候也 会做类似的事情。当程序调用一个对象的方法时，虚拟机需要进行动态绑定，他必须判断调用哪一个类型的方法。这也需要做上面的判断。 无论虚拟机实现者使用哪一种设计，他都可能为每一个对象保存一个类似方法列表的信息。因为他可以提升对象方法调用的速度，对提升虚拟机的性能非常重要，但 是虚拟机的规范中比没有要求必须实现类似的数据结构。下图描述了这种结构。图中显示了一个对象引用相关联的所有的数据结构，包括： 1）、一个指向类型数据的指针 2）、一个对象的方法列表。方法列表是一个指向所有可能被调用对象方法的指针数组。方法数据包括三个部分：操作码堆栈的大小和方法堆栈的本地变量区；方法的字节码；异常列表。 每一个Java虚拟机中的对象必须关联一个用于同步多线程的lock(mutex)。同一时刻，只能有一个对象拥有这个对象的锁。当一个拥有这个这个对象 的锁，他就可以多次申请这个锁，但是也必须释放相应次数的锁才能真正释放这个对象锁。很多对象在整个生命周期中都不会被锁，所以这个信息只有在需要时才需 要添加。很多Java虚拟机的实现都没有在对象的数据中包含“锁定数据”，只是在需要时才生成相应的数据。除了实现对象的锁定，每一个对象还逻辑关联到一 个“wait set”的实现。锁定帮组线程独立处理共享的数据，不需要妨碍其他的线程。“wait set”帮组线程协作完成同一个目标。“wait set”往往通过Object类的wait()和notify()方法来实现。 垃圾收集也需要堆中的对象是否被关联的信息。Java虚拟机规范中指出垃圾收集一个运行一个对象的finalizer方法一次，但是容许 finalizer方法重新引用这个对象，当这个对象再次不被引用时，就不需要再次调用finalize方法。所以虚拟机也需要保存finalize方法 是否运行过的信息。更多信息参见第九章的“垃圾收集” 3、数组的保存（Array Representation）在Java 中，数组是一种完全意义上的对象，他和对象一样保存在堆中、有一个指向Class类实例的引用。所有同一维度和类型的数组拥有同样的Class，数组的长 度不做考虑。对应Class的名字表示为维度和类型。比如一个整型数据的Class为“[I”，字节型三维数组Class名为“[[[B”，两维对象数据 Class名为“[[Ljava.lang.Object”。 数组必须在堆中保存数组的长度，数组的数据和一些对象数组类型数据的引用。通过一个数组引用的，虚拟机应该能够取得一个数组的长度，通过索引能够访问特定 的数据，能够调用Object定义的方法。Object是所有数据类的直接父类。更多信息参见第六章“类文件”。 六.基本结构从Java平台的逻辑结构上来看，我们可以从下图来了解JVM： 从上图能清晰看到Java平台包含的各个逻辑模块，也能了解到JDK与JRE的区别。 JVM自身的物理结构 此图看出jvm内存结构 JVM内存结构主要包括两个子系统和两个组件。两个子系统分别是Classloader子系统和Executionengine(执行引擎)子系统；两个组件分别是Runtimedataarea(运行时数据区域)组件和Nativeinterface(本地接口)组件。 Classloader子系统的作用： 根据给定的全限定名类名(如java.lang.Object)来装载class文件的内容到Runtimedataarea中的methodarea(方法区域)。Java程序员可以extendsjava.lang.ClassLoader类来写自己的Classloader。 Executionengine子系统的作用： 执行classes中的指令。任何JVMspecification实现(JDK)的核心都是Executionengine，不同的JDK例如Sun的JDK和IBM的JDK好坏主要就取决于他们各自实现的Executionengine的好坏。 Nativeinterface组件： 与nativelibraries交互，是其它编程语言交互的接口。当调用native方法的时候，就进入了一个全新的并且不再受虚拟机限制的世界，所以也很容易出现JVM无法控制的nativeheapOutOfMemory。 RuntimeDataArea组件： 这就是我们常说的JVM的内存了。它主要分为五个部分—— 1、Heap(堆)：一个Java虚拟实例中只存在一个堆空间 2、MethodArea(方法区域)：被装载的class的信息存储在Methodarea的内存中。当虚拟机装载某个类型时，它使用类装载器定位相应的class文件，然后读入这个class文件内容并把它传输到虚拟机中。 3、JavaStack(java的栈)：虚拟机只会直接对Javastack执行两种操作：以帧为单位的压栈或出栈 4、ProgramCounter(程序计数器)：每一个线程都有它自己的PC寄存器，也是该线程启动时创建的。PC寄存器的内容总是指向下一条将被执行指令的饿地址，这里的地址可以是一个本地指针，也可以是在方法区中相对应于该方法起始指令的偏移量。 5、Nativemethodstack(本地方法栈)：保存native方法进入区域的地址 对于JVM的学习，在我看来这么几个部分最重要： Java代码编译和执行的整个过程 JVM内存管理及垃圾回收机制 Java代码编译是由Java源码编译器来完成，流程图如下所示： Java字节码的执行是由JVM执行引擎来完成，流程图如下所示： Java代码编译和执行的整个过程包含了以下三个重要的机制： Java源码编译机制 类加载机制 类执行机制 七.Java源码编译机制Java 源码编译由以下三个过程组成：（javac –verbose 输出有关编译器正在执行的操作的消息） 分析和输入到符号表 注解处理 语义分析和生成class文件 最后生成的class文件由以下部分组成： 结构信息。包括class文件格式版本号及各部分的数量与大小的信息 元数据。对应于Java源码中声明与常量的信息。包含类/继承的超类/实现的接口的声明信息、域与方法声明信息和常量池 方法信息。对应Java源码中语句和表达式对应的信息。包含字节码、异常处理器表、求值栈与局部变量区大小、求值栈的类型记录、调试符号信息 八.类加载机制JVM的类加载是通过ClassLoader及其子类来完成的，类的层次关系和加载顺序可以由下图来描述： 1）Bootstrap ClassLoader /启动类加载器 $JAVA_HOME中jre/lib/rt.jar里所有的class，由C++实现，不是ClassLoader子类 2）Extension ClassLoader/扩展类加载器 负责加载java平台中扩展功能的一些jar包，包括$JAVA_HOME中jre/lib/*.jar或-Djava.ext.dirs指定目录下的jar包 3）App ClassLoader/ 系统类加载器 负责记载classpath中指定的jar包及目录中class 4）Custom ClassLoader/用户自定义类加载器(java.lang.ClassLoader的子类) 属于应用程序根据自身需要自定义的ClassLoader，如tomcat、jboss都会根据j2ee规范自行实现ClassLoader 加载过程中会先检查类是否被已加载，检查顺序是自底向上，从Custom ClassLoader到BootStrap ClassLoader逐层检查，只要某个classloader已加载就视为已加载此类，保证此类只所有ClassLoader加载一次。而加载的顺序是自顶向下，也就是由上层来逐层尝试加载此类。 九.类加载双亲委派机制介绍和分析在这里，需要着重说明的是，JVM在加载类时默认采用的是双亲委派机制。通俗的讲，就是某个特定的类加载器在接到加载类的请求时，首先将加载任务委托给父类加载器，依次递归，如果父类加载器可以完成类加载任务，就成功返回；只有父类加载器无法完成此加载任务时，才自己去加载。 十.类执行机制JVM是基于栈的体系结构来执行class字节码的。线程创建后，都会产生程序计数器（PC）和栈（Stack），程序计数器存放下一条要执行的指令在方法内的偏移量，栈中存放一个个栈帧，每个栈帧对应着每个方法的每次调用，而栈帧又是有局部变量区和操作数栈两部分组成，局部变量区用于存放方法中的局部变量和参数，操作数栈中用于存放方法执行过程中产生的中间结果。 十一.JVM内存组成结构JVM栈由堆、栈、本地方法栈、方法区等部分组成，结构图如下所示： 十二.JVM内存回收Sun的JVMGenerationalCollecting(垃圾回收)原理是这样的：把对象分为年青代(Young)、年老代(Tenured)、持久代(Perm)，对不同生命周期的对象使用不同的算法。(基于对对象生命周期分析) 1.Young（年轻代） 年轻代分三个区。一个Eden区，两个Survivor区。大部分对象在Eden区中生成。当Eden区满时，还存活的对象将被复制到Survivor区（两个中的一个），当这个Survivor区满时，此区的存活对象将被复制到另外一个Survivor区，当这个Survivor去也满了的时候，从第一个Survivor区复制过来的并且此时还存活的对象，将被复制年老区(Tenured。需要注意，Survivor的两个区是对称的，没先后关系，所以同一个区中可能同时存在从Eden复制过来对象，和从前一个Survivor复制过来的对象，而复制到年老区的只有从第一个Survivor去过来的对象。而且，Survivor区总有一个是空的。 2.Tenured（年老代） 年老代存放从年轻代存活的对象。一般来说年老代存放的都是生命期较长的对象。 3.Perm（持久代） 用于存放静态文件，如今Java类、方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如hibernate等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。持久代大小通过-XX:MaxPermSize=进行设置。 举个例子：当在程序中生成对象时，正常对象会在年轻代中分配空间，如果是过大的对象也可能会直接在年老代生成（据观测在运行某程序时候每次会生成一个十兆的空间用收发消息，这部分内存就会直接在年老代分配）。年轻代在空间被分配完的时候就会发起内存回收，大部分内存会被回收，一部分幸存的内存会被拷贝至Survivor的from区，经过多次回收以后如果from区内存也分配完毕，就会也发生内存回收然后将剩余的对象拷贝至to区。等到to区也满的时候，就会再次发生内存回收然后把幸存的对象拷贝至年老区。 通常我们说的JVM内存回收总是在指堆内存回收，确实只有堆中的内容是动态申请分配的，所以以上对象的年轻代和年老代都是指的JVM的Heap空间，而持久代则是之前提到的MethodArea，不属于Heap。 十三.关于JVM内存管理的一些建议1、手动将生成的无用对象，中间对象置为null，加快内存回收。 2、对象池技术如果生成的对象是可重用的对象，只是其中的属性不同时，可以考虑采用对象池来较少对象的生成。如果有空闲的对象就从对象池中取出使用，没有再生成新的对象，大大提高了对象的复用率。 3、JVM调优通过配置JVM的参数来提高垃圾回收的速度，如果在没有出现内存泄露且上面两种办法都不能保证JVM内存回收时，可以考虑采用JVM调优的方式来解决，不过一定要经过实体机的长期测试，因为不同的参数可能引起不同的效果。如-Xnoclassgc参数等。]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式事物的原理及解决方案]]></title>
    <url>%2F2018%2F11%2F20%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E7%89%A9%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[作者：李占卫 链接：https://juejin.im/post/5bf379b4e51d457e052fe5e0 来源：掘金 1.引言1分布式事务是企业集成中的一个技术难点，也是每一个分布式系统架构中都会涉及到的一个东西，特别是在这几年越来越火的微服务架构中，几乎可以说是无法避免，本文就围绕单机事务，分布式事务以及分布式事务的处理方式来展开。 2.事物事务提供一种“要么什么都不做，要么做全套（All or Nothing）”的机制，她有ACID四大特性 原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。 一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态是指数据库中的数据应满足完整性约束。除此之外，一致性还有另外一层语义，就是事务的中间状态不能被观察到（这层语义也有说应该属于原子性）。 隔离性（Isolation）：多个事务并发执行时，一个事务的执行不应影响其他事务的执行，如同只有这一个操作在被数据库所执行一样。 持久性（Durability）：已被提交的事务对数据库的修改应该永久保存在数据库中。在事务结束时，此操作将不可逆转 2.1单机事物 以mysql的InnoDB存储引擎为例，来了解单机事务是如何保证ACID特性的。 事务的隔离性是通过数据库锁的机制实现的，持久性通过redo log（重做日志）来实现，原子性和一致性通过Undo log来实现。 2.2分布式事物单机事务是通过将操作限制在一个会话内通过数据库本身的锁以及日志来实现ACID，那么分布式环境下该如何保证ACID特性那? 2.2.1 XA协议实现分布式事物2.2.1.1 XA描述X/Open DTP(X/Open Distributed Transaction Processing Reference Model) 是X/Open 这个组织定义的一套分布式事务的标准，也就是了定义了规范和API接口，由各个厂商进行具体的实现。 X/Open DTP 定义了三个组件： AP，TM，RM AP(Application Program)：也就是应用程序，可以理解为使用DTP的程序 RM(Resource Manager)：资源管理器，这里可以理解为一个DBMS系统，或者消息服务器管理系统，应用程序通过资源管理器对资源进行控制。资源必须实现XA定义的接口 TM(Transaction Manager)：事务管理器，负责协调和管理事务，提供给AP应用程序编程接口以及管理资源管理器 其中在DTP定义了以下几个概念 事务：一个事务是一个完整的工作单元，由多个独立的计算任务组成，这多个任务在逻辑上是原子的 全局事务：对于一次性操作多个资源管理器的事务，就是全局事务 分支事务：在全局事务中，某一个资源管理器有自己独立的任务，这些任务的集合作为这个资源管理器的分支任务 控制线程：用来表示一个工作线程，主要是关联AP,TM,RM三者的一个线程，也就是事务上下文环境。简单的说，就是需要标识一个全局事务以及分支事务的关系 如果一个事务管理器管理着多个资源管理器，DTP是通过两阶段提交协议来控制全局事务和分支事务。 第一阶段：准备阶段 事务管理器通知资源管理器准备分支事务，资源管理器告之事务管理器准备结果 第二阶段：提交阶段 事务管理器通知资源管理器提交分支事务，资源管理器告之事务管理器结果 2.2.1.2 XA的ACID特性 原子性：XA议使用2PC原子提交协议来保证分布式事务原子性 隔离性：XA要求每个RMs实现本地的事务隔离，子事务的隔离来保证整个事务的隔离。 一致性：通过原子性、隔离性以及自身一致性的实现来保证“数据库从一个一致状态转变为另一个一致状态”；通过MVCC来保证中间状态不能被观察到。 2.2.1.3 XA的优缺点优点： 对业务无侵入，对RM要求高 缺点： 同步阻塞：在二阶段提交的过程中，所有的节点都在等待其他节点的响应，无法进行其他操作。这种同步阻塞极大的限制了分布式系统的性能。 单点问题：协调者在整个二阶段提交过程中很重要，如果协调者在提交阶段出现问题，那么整个流程将无法运转。更重要的是，其他参与者将会处于一直锁定事务资源的状态中，而无法继续完成事务操作。 数据不一致：假设当协调者向所有的参与者发送commit请求之后，发生了局部网络异常，或者是协调者在尚未发送完所有 commit请求之前自身发生了崩溃，导致最终只有部分参与者收到了commit请求。这将导致严重的数据不一致问题。 容错性不好：如果在二阶段提交的提交询问阶段中，参与者出现故障，导致协调者始终无法获取到所有参与者的确认信息，这时协调者只能依靠其自身的超时机制，判断是否需要中断事务。显然，这种策略过于保守。换句话说，二阶段提交协议没有设计较为完善的容错机制，任意一个节点是失败都会导致整个事务的失败。 2.2.2 TCC协议实现分布式事物2.2.2.1 TCC描述TCC（Try-Confirm-Cancel）分布式事务模型相对于 XA 等传统模型，其特征在于它不依赖资源管理器（RM）对分布式事务的支持，而是通过对业务逻辑的分解来实现分布式事务。 第一阶段：CanCommit 3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。 事务询问：协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应 响应反馈：参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态；否则反馈No。 第二阶段：PreCommit 协调者在得到所有参与者的响应之后，会根据结果执行2种操作：执行事务预提交，或者中断事务 执行事务预提交 发送预提交请求：协调者向所有参与者节点发出 preCommit 的请求，并进入 prepared 状态。 事务预提交：参与者受到 preCommit 请求后，会执行事务操作，对应 2PC 准备阶段中的 “执行事务”，也会 Undo 和 Redo 信息记录到事务日志中。 各参与者响应反馈:如果参与者成功执行了事务，就反馈 ACK 响应，同时等待指令：提交（commit） 或终止（abort） 中断事务 发送中断请求：协调者向所有参与者节点发出 abort 请求 。 中断事务：参与者如果收到 abort 请求或者超时了，都会中断事务。 第三阶段：Do Commit 该阶段进行真正的事务提交，也可以分为以下两种情况 执行提交 发送提交请求：协调者接收到各参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送 doCommit 请求。 事务提交：参与者接收到 doCommit 请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。 响应反馈：事务提交完之后，向协调者发送 ACK 响应。 完成事务：协调者接收到所有参与者的 ACK 响应之后，完成事务。 中断事务 协调者没有接收到参与者发送的 ACK 响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。 发送中断请求：协调者向所有参与者发送 abort 请求。 事务回滚：参与者接收到 abort 请求之后，利用其在阶段二记录的 undo 信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。 反馈结果：参与者完成事务回滚之后，向协调者发送 ACK 消息。 中断事务：协调者接收到参与者反馈的 ACK 消息之后，完成事务的中断。 2.2.2.2 TCC的ACID特性原子性：TCC 模型也使用 2PC 原子提交协议来保证事务原子性。Try 操作对应2PC 的一阶段准备（Prepare）；Confirm 对应 2PC 的二阶段提交（Commit），Cancel 对应 2PC 的二阶段回滚（Rollback），可以说 TCC 就是应用层的 2PC。 隔离性：隔离的本质是控制并发，放弃在数据库层面加锁通过在业务层面加锁来实现。【比如在账户管理模块设计中，增加可用余额和冻结金额的设置】 一致性：通过原子性保证事务的原子提交、业务隔离性控制事务的并发访问，实现分布式事务的一致性状态转变；事务的中间状态不能被观察到这点并不保证[本协议是基于柔性事务理论提出的]。。 2.2.2.3 TCC的优缺点优点： 相对于二阶段提交，三阶段提交主要解决的单点故障问题，并减少了阻塞的时间。因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行 commit。而不会一直持有事务资源并处于阻塞状态。 缺点： 三阶段提交也会导致数据一致性问题。由于网络原因，协调者发送的 Cancel 响应没有及时被参与者接收到，那么参与者在等待超时之后执行了 commit 操作。这样就和其他接到 Cancel 命令并执行回滚的参与者之间存在数据不一致的情况。 2.2.3 SAGA协议实现分布式事物2.2.3.1 SAGA协议介绍Saga的组成： 每个Saga由一系列sub-transaction Ti 组成 每个Ti 都有对应的补偿动作Ci，补偿动作用于撤销Ti造成的结果 saga的执行顺序有两种： T1, T2, T3, …, Tn T1, T2, …, Tj, Cj,…, C2, C1，其中0 &lt; j &lt; n Saga定义了两种恢复策略： backward recovery，向后恢复，即上面提到的第二种执行顺序，其中j是发生错误的sub-transaction，这种做法的效果是撤销掉之前所有成功的sub-transation，使得整个Saga的执行结果撤销。 forward recovery，向前恢复，适用于必须要成功的场景，执行顺序是类似于这样的：T1, T2, …, Tj(失败), Tj(重试),…, Tn，其中j是发生错误的sub-transaction。该情况下不需要Ci。 Saga的注意事项 Ti和Ci是幂等的。举个例子，假设在执行Ti的时候超时了，此时我们是不知道执行结果的，如果采用forward recovery策略就会再次发送Ti，那么就有可能出现Ti被执行了两次，所以要求Ti幂等。如果采用backward recovery策略就会发送Ci，而如果Ci也超时了，就会尝试再次发送Ci，那么就有可能出现Ci被执行两次，所以要求Ci幂等。 Ci必须是能够成功的，如果无法成功则需要人工介入。如果Ci不能执行成功就意味着整个Saga无法完全撤销，这个是不允许的。但总会出现一些特殊情况比如Ci的代码有bug、服务长时间崩溃等，这个时候就需要人工介入了 Ti - Ci和Ci - Ti的执行结果必须是一样的：sub-transaction被撤销了。举例说明，还是考虑Ti执行超时的场景，我们采用了backward recovery，发送一个Ci，那么就会有三种情况： 1：Ti的请求丢失了，服务之前没有、之后也不会执行Ti 2：Ti在Ci之前执行 3：Ci在Ti之前执行 对于第1种情况，容易处理。对于第2、3种情况，则要求Ti和Ci是可交换的（commutative)，并且其最终结果都是sub-transaction被撤销。 Saga架构 Saga Execution Component解析请求JSON并构建请求图 TaskRunner 用任务队列确保请求的执行顺序 TaskConsumer 处理Saga任务，将事件写入saga log，并将请求发送到远程服务 2.2.3.2 SAGA的ACID特性 原子性：通过SAGA协调器实现 一致性：本地事务+SAGA Log 持久性：SAGA Log 隔离性：不保证（同TCC） 3.分布式事物的处理方案3.1 XA仅在同一个事务上下文中需要协调多种资源（即数据库，以及消息主题或队列）时，才有必要使用 X/Open XA 接口。数据库接入XA需要使用XA版的数据库驱动，消息队列要实现XA需要实现javax.transaction.xa.XAResource接口。 3.1.1 jotm的分布式事务代码如下: 12345678910111213141516171819202122232425262728public class UserService &#123; @Autowired private UserDao userDao; @Autowired private LogDao logDao; @Transactional public void save(User user)&#123; userDao.save(user); logDao.save(user); throw new RuntimeException(); &#125;&#125;@Resourcepublic class UserDao &#123; @Resource(name="jdbcTemplateA") private JdbcTemplate jdbcTemplate; public void save(User user)&#123; jdbcTemplate.update("insert into user(name,age) values(?,?)",user.getName(),user.getAge()); &#125;&#125;@Repositorypublic class LogDao &#123; @Resource(name="jdbcTemplateB") private JdbcTemplate jdbcTemplate; public void save(User user)&#123; jdbcTemplate.update("insert into log(name,age) values(?,?)",user.getName(),user.getAge()); &#125;&#125; 配置： 1234567891011121314151617181920212223242526272829303132333435&lt;bean id="jotm" class="org.objectweb.jotm.Current" /&gt;&lt;bean id="transactionManager" class="org.springframework.transaction.jta.JtaTransactionManager"&gt; &lt;property name="userTransaction" ref="jotm" /&gt;&lt;/bean&gt;&lt;tx:annotation-driven transaction-manager="transactionManager"/&gt;&lt;!-- 配置数据源 --&gt;&lt;bean id="dataSourceA" class="org.enhydra.jdbc.pool.StandardXAPoolDataSource" destroy-method="shutdown"&gt; &lt;property name="dataSource"&gt; &lt;bean class="org.enhydra.jdbc.standard.StandardXADataSource" destroy-method="shutdown"&gt; &lt;property name="transactionManager" ref="jotm" /&gt; &lt;property name="driverName" value="com.mysql.jdbc.Driver" /&gt; &lt;property name="url" value="jdbc:mysql://localhost:3306/test?useUnicode=true&amp;amp;characterEncoding=utf-8" /&gt; &lt;/bean&gt; &lt;/property&gt; &lt;property name="user" value="xxx" /&gt; &lt;property name="password" value="xxx" /&gt;&lt;/bean&gt;&lt;!-- 配置数据源 --&gt;&lt;bean id="dataSourceB" class="org.enhydra.jdbc.pool.StandardXAPoolDataSource" destroy-method="shutdown"&gt; &lt;property name="dataSource"&gt; &lt;bean class="org.enhydra.jdbc.standard.StandardXADataSource" destroy-method="shutdown"&gt; &lt;property name="transactionManager" ref="jotm" /&gt; &lt;property name="driverName" value="com.mysql.jdbc.Driver" /&gt; &lt;property name="url" value="jdbc:mysql://localhost:3306/test2?useUnicode=true&amp;amp;characterEncoding=utf-8" /&gt; &lt;/bean&gt; &lt;/property&gt; &lt;property name="user" value="xxx" /&gt; &lt;property name="password" value="xxx" /&gt;&lt;/bean&gt;&lt;bean id="jdbcTemplateA" class="org.springframework.jdbc.core.JdbcTemplate"&gt; &lt;property name="dataSource" ref="dataSourceA" /&gt;&lt;/bean&gt;&lt;bean id="jdbcTemplateB" class="org.springframework.jdbc.core.JdbcTemplate"&gt; &lt;property name="dataSource" ref="dataSourceB" /&gt;&lt;/bean&gt; 使用到的JAR包: 123compile &apos;org.ow2.jotm:jotm-core:2.3.1-M1&apos;compile &apos;org.ow2.jotm:jotm-datasource:2.3.1-M1&apos;compile &apos;com.experlog:xapool:1.5.0&apos; 事务配置: 我们知道分布式事务中需要一个事务管理器即接口javax.transaction.TransactionManager、面向开发人员的javax.transaction.UserTransaction。对于jotm来说，他们的实现类都是Current public class Current implements UserTransaction, TransactionManager 我们如果想使用分布式事务的同时，又想使用Spring带给我们的@Transactional便利，就需要配置一个JtaTransactionManager，而该JtaTransactionManager是需要一个userTransaction实例的，所以用到了上面的Current，如下配置: 12345&lt;bean id="jotm" class="org.objectweb.jotm.Current" /&gt;&lt;bean id="transactionManager" class="org.springframework.transaction.jta.JtaTransactionManager"&gt; &lt;property name="userTransaction" ref="jotm" /&gt; &lt;/bean&gt;&lt;tx:annotation-driven transaction-manager="transactionManager"/&gt; 执行过程： 第一步：事务拦截器开启事务 我们知道加入了@Transactional注解，同时开启tx:annotation-driven，会对本对象进行代理，加入事务拦截器。在事务拦截器中，获取javax.transaction.UserTransaction，这里即org.objectweb.jotm.Current，然后使用它开启事务，并和当前线程进行绑定，绑定关系数据存放在org.objectweb.jotm.Current中。 第二步：使用jdbcTemplate进行业务操作 dbcTemplateA要从dataSourceA中获取Connection,和当前线程进行绑定，同时以对应的dataSourceA作为key。同时判断当前线程是否含有事务，通过dataSourceA中的org.objectweb.jotm.Current发现当前线程有事务，则把Connection自动提交设置为false,同时将该连接纳入当前事务中。 jdbcTemplateB要从dataSourceB中获取Connection,和当前线程进行绑定，同时以对应的dataSourceB作为key。同时判断当前线程是否含有事务，通过dataSourceB中的org.objectweb.jotm.Current发现当前线程有事务，则把Connection自动提交设置为false,同时将该连接纳入当前事务中。 第三步：异常回滚 一旦抛出异常，则需要进行事务的回滚操作。回滚就是将当前事务进行回滚，该事务的回滚会调用和它关联的所有Connection的回滚。 3.1.2 Atomikos的分布式事务代码同上，配置如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;bean id="atomikosUserTransaction" class="com.atomikos.icatch.jta.UserTransactionImp"&gt; &lt;property name="transactionTimeout" value="300" /&gt;&lt;/bean&gt;&lt;bean id="springTransactionManager" class="org.springframework.transaction.jta.JtaTransactionManager"&gt; &lt;property name="userTransaction" ref="atomikosUserTransaction" /&gt;&lt;/bean&gt;&lt;tx:annotation-driven transaction-manager="springTransactionManager"/&gt;&lt;!-- 配置数据源 --&gt;&lt;bean id="dataSourceC" class="com.atomikos.jdbc.AtomikosDataSourceBean" init-method="init" destroy-method="close"&gt; &lt;property name="uniqueResourceName" value="XA1DBMS" /&gt; &lt;property name="xaDataSourceClassName" value="com.mysql.jdbc.jdbc2.optional.MysqlXADataSource" /&gt; &lt;property name="xaProperties"&gt; &lt;props&gt; &lt;prop key="URL"&gt;jdbc:mysql://localhost:3306/test?useUnicode=true&amp;amp;characterEncoding=utf-8&lt;/prop&gt; &lt;prop key="user"&gt;xxx&lt;/prop&gt; &lt;prop key="password"&gt;xxx&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;property name="poolSize" value="3" /&gt; &lt;property name="minPoolSize" value="3" /&gt; &lt;property name="maxPoolSize" value="5" /&gt;&lt;/bean&gt;&lt;!-- 配置数据源 --&gt;&lt;bean id="dataSourceD" class="com.atomikos.jdbc.AtomikosDataSourceBean" init-method="init" destroy-method="close"&gt; &lt;property name="uniqueResourceName" value="XA2DBMS" /&gt; &lt;property name="xaDataSourceClassName" value="com.mysql.jdbc.jdbc2.optional.MysqlXADataSource" /&gt; &lt;property name="xaProperties"&gt; &lt;props&gt; &lt;prop key="URL"&gt;jdbc:mysql://localhost:3306/test2?useUnicode=true&amp;amp;characterEncoding=utf-8&lt;/prop&gt; &lt;prop key="user"&gt;xxx&lt;/prop&gt; &lt;prop key="password"&gt;xxx&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;property name="poolSize" value="3" /&gt; &lt;property name="minPoolSize" value="3" /&gt; &lt;property name="maxPoolSize" value="5" /&gt;&lt;/bean&gt;&lt;bean id="jdbcTemplateC" class="org.springframework.jdbc.core.JdbcTemplate"&gt; &lt;property name="dataSource" ref="dataSourceC" /&gt;&lt;/bean&gt;&lt;bean id="jdbcTemplateD" class="org.springframework.jdbc.core.JdbcTemplate"&gt; &lt;property name="dataSource" ref="dataSourceD" /&gt;&lt;/bean&gt; 事务配置： 我们知道分布式事务中需要一个事务管理器即接口javax.transaction.TransactionManager、面向开发人员的javax.transaction.UserTransaction。对于Atomikos来说分别对应如下： com.atomikos.icatch.jta.UserTransactionImp com.atomikos.icatch.jta.UserTransactionManager 我们如果想使用分布式事务的同时，又想使用Spring带给我们的@Transactional便利，就需要配置一个JtaTransactionManager，而该JtaTransactionManager是需要一个userTransaction实例的 1234567&lt;bean id="userTransaction" class="com.atomikos.icatch.jta.UserTransactionImp"&gt; &lt;property name="transactionTimeout" value="300" /&gt; &lt;/bean&gt;&lt;bean id="springTransactionManager" class="org.springframework.transaction.jta.JtaTransactionManager"&gt; &lt;property name="userTransaction" ref="userTransaction" /&gt; &lt;/bean&gt;&lt;tx:annotation-driven transaction-manager="springTransactionManager"/&gt; 可以对比下jotm的案例配置jotm的分布式事务配置。可以看到jotm中使用的xapool中的StandardXADataSource是需要一个transactionManager的，而Atomikos使用的AtomikosNonXADataSourceBean则不需要。我们知道，StandardXADataSource中有了transactionManager就可以获取当前线程的事务，同时把XAResource加入进当前事务中去，而AtomikosNonXADataSourceBean却没有，它是怎么把XAResource加入进当前线程绑定的事务呢？这时候就需要可以通过静态方法随时获取当前线程绑定的事务。 使用到的JAR包: 1compile 'com.atomikos:transactions-jdbc:4.0.0M4' 3.2 单机事物 + 同步（异步）回调以订单子系统和支付子系统为例，如下图： 如上图，payment是支付系统，trade是订单系统，两个系统对应的数据库是分开的。支付完成之后，支付系统需要通知订单系统状态变更。对于payment要执行的操作可以用伪代码表示如下： 12345begin tx; count = update account set amount = amount - $&#123;cash&#125; where uid = $&#123;uid&#125; and amount &gt;= amount if (count &lt;= 0) return false update payment_record set status = paid where trade_id = $&#123;tradeId&#125;commit; 对于trade要执行的操作可以用伪代码表示如下： 12345begin tx; count = update trade_record set status = paid where trade_id = $&#123;trade_id&#125; and status = unpaid if (count &lt;= 0) return false do other things ...commit; 但是对于这两段代码如何串起来是个问题，我们增加一个事务表，即图中的tx_info，来记录成功完成的支付事务，tx_info中需要有可以标示被支付系统处理状态的字段，为了和支付信息一致，需要放入事务中，代码如下： 123456begin tx; count = update account set amount = amount - $&#123;cash&#125; where uid = $&#123;uid&#125; and amount &gt;= amount if (count &lt;= 0) return false update payment_record set status = paid where trade_id = $&#123;tradeId&#125; insert into tx_info values($&#123;trade_id&#125;,$&#123;amount&#125;...)commit; 支付系统边界到此为止，接下来就是订单系统轮询访问tx_info，拉取已经支付成功的订单信息，对每一条信息都执行trade系统的逻辑，伪代码如下： 123foreach trade_id in tx_info do trade_tx save tx_info.id to some store 事无延迟取决于时间程序轮询间隔，这样我们做到了一致性，最终订单都会在支付之后的最大时间间隔内完成状态迁移。 当然，这里也可以采用支付系统通过RPC方式同步通知订单系统的方式来实现，处理状态通过tx_info中的字段来表示。 另外，交易系统每次拉取数据的起点以及消费记录需要记录下来，这样才能不遗漏不重复地执行，所以需要增加一张表用于排重，即上图中的tx_duplication。但是每次对tx_duplication表的插入要在trade_tx的事务中完成，伪代码如下： 1234567begin tx; c = insert ignore tx_duplication values($trade_id...) if (c &lt;= 0) return false count = update trade_record set status = paid where trade_id = $&#123;trade_id&#125; and status = unpaid if (count &lt;= 0) return false do other things ...commit; 另外，tx_duplication表中trade_id表上必须有唯一键，这个算是结合之前的幂等篇来保证trade_tx的操作是幂等的。 3.3MQ做中间表角色在上面的方案中，tx_info表所起到的作用就是队列作用，记录一个系统的表更，作为通知给需要感知的系统的事件。而时间程序去拉取只是系统去获取感兴趣事件的一个方式，而对应交易系统的本地事务只是对应消费事件的一个过程。在这样的描述下，这些功能就是一个MQ——消息中间件。如下图 这样tx_info表的功能就交给了MQ，消息消费的偏移量也不需要关心了，MQ会搞定的，但是tx_duplication还是必须存在的，因为MQ并不能避免消息的重复投递，这其中的原因有很多，主要是还是分布式的CAP造成的，再次不详细描述。 这要求MQ必须支持事务功能，可以达到本地事务和消息发出是一致性的，但是不必是强一致的。通常使用的方式如下的伪代码： 1234sendPrepare();isCommit = local_tx()if (isCommit) sendCommit() else sendRollback() 在做本地事务之前，先向MQ发送一个prepare消息，然后执行本地事务，本地事务提交成功的话，向MQ发送一个commit消息，否则发送一个abort消息，取消之前的消息。MQ只会在收到commit确认才会将消息投递出去，所以这样的形式可以保证在一切正常的情况下，本地事务和MQ可以达到一致性。 但是分布式存在异常情况，网络超时，机器宕机等等，比如当系统执行了local_tx()成功之后，还没来得及将commit消息发送给MQ，或者说发送出去了，网络超时了等等原因，MQ没有收到commit，即commit消息丢失了，那么MQ就不会把prepare消息投递出去。如果这个无法保证的话，那么这个方案是不可行的。针对这种情况，需要一个第三方异常校验模块来对MQ中在一定时间段内没有commit/abort 的消息和发消息的系统进行检查，确认该消息是否应该投递出去或者丢弃，得到系统的确认之后，MQ会做投递还是丢弃，这样就完全保证了MQ和发消息的系统的一致性，从而保证了接收消息系统的一致性。 这个方案要求MQ的系统可用性必须非常高，至少要超过使用MQ的系统（推荐rocketmq，kafka都支持发送预备消息和业务回查），这样才能保证依赖他的系统能稳定运行。 3.4 SAGA方案项目地址：github.com/apache/serv… Saga处理场景是要求相关的子事务提供事务处理函数同时也提供补偿函数。Saga协调器alpha会根据事务的执行情况向omega发送相关的指令，确定是否向前重试或者向后恢复。 成功场景成功场景下，每个事务都会有开始和有对应的结束事件。 异常场景异常场景下，omega会向alpha上报中断事件，然后alpha会向该全局事务的其它已完成的子事务发送补偿指令，确保最终所有的子事务要么都成功，要么都回滚。 超市场景超时场景下，已超时的事件会被alpha的定期扫描器检测出来，与此同时，该超时事务对应的全局事务也会被中断。 例子假设要租车、预订酒店满足分布式事务。租车服务 12345678910111213141516171819202122232425@Serviceclass CarBookingService &#123; private Map&lt;Integer, CarBooking&gt; bookings = new ConcurrentHashMap&lt;&gt;(); @Compensable(compensationMethod = "cancel") void order(CarBooking booking) &#123; booking.confirm(); bookings.put(booking.getId(), booking); &#125; void cancel(CarBooking booking) &#123; Integer id = booking.getId(); if (bookings.containsKey(id)) &#123; bookings.get(id).cancel(); &#125; &#125; Collection&lt;CarBooking&gt; getAllBookings() &#123; return bookings.values(); &#125; void clearAllBookings() &#123; bookings.clear(); &#125;&#125; 酒店预订 12345678910111213141516171819202122232425262728@Serviceclass HotelBookingService &#123; private Map&lt;Integer, HotelBooking&gt; bookings = new ConcurrentHashMap&lt;&gt;(); @Compensable(compensationMethod = "cancel") void order(HotelBooking booking) &#123; if (booking.getAmount() &gt; 2) &#123; throw new IllegalArgumentException("can not order the rooms large than two"); &#125; booking.confirm(); bookings.put(booking.getId(), booking); &#125; void cancel(HotelBooking booking) &#123; Integer id = booking.getId(); if (bookings.containsKey(id)) &#123; bookings.get(id).cancel(); &#125; &#125; Collection&lt;HotelBooking&gt; getAllBookings() &#123; return bookings.values(); &#125; void clearAllBookings() &#123; bookings.clear(); &#125;&#125; 主服务 12345678910111213141516171819202122232425262728293031323334353637383940@RestControllerpublic class BookingController &#123; @Value("$&#123;car.service.address:http://car.servicecomb.io:8080&#125;") private String carServiceUrl; @Value("$&#123;hotel.service.address:http://hotel.servicecomb.io:8080&#125;") private String hotelServiceUrl; @Autowired private RestTemplate template; @SagaStart @PostMapping("/booking/&#123;name&#125;/&#123;rooms&#125;/&#123;cars&#125;") public String order(@PathVariable String name, @PathVariable Integer rooms, @PathVariable Integer cars) &#123; template.postForEntity( carServiceUrl + "/order/&#123;name&#125;/&#123;cars&#125;", null, String.class, name, cars); postCarBooking(); template.postForEntity( hotelServiceUrl + "/order/&#123;name&#125;/&#123;rooms&#125;", null, String.class, name, rooms); postBooking(); return name + " booking " + rooms + " rooms and " + cars + " cars OK"; &#125; // This method is used by the byteman to inject exception here private void postCarBooking() &#123; &#125; // This method is used by the byteman to inject the faults such as the timeout or the crash private void postBooking() &#123; &#125;&#125; 执行流程 在Alpha目录执行 mvn clean package -DskipTests -Pdemo 执行 java -Dspring.profiles.active=prd -D”spring.datasource.url=jdbc:postgresql://host_address:5432/saga?useSSL=false” -jar alpha-server-](https://juejin.im/equation?tex=%7Bhost_address%7D%3A5432%2Fsaga%3FuseSSL%3Dfalse%22%20-jar%20alpha-server-){saga_version}-exec.jar 在saga spring demo目录执行 mvn clean package -DskipTests -Pdemo java -Dserver.port=8081 -Dalpha.cluster.address=alpha_address:8080 -jar hotel-](https://juejin.im/equation?tex=%7Balpha_address%7D%3A8080%20-jar%20hotel-){saga_version}-exec.jar java -Dserver.port=8082 -Dalpha.cluster.address=alpha_address:8080 -jar car-](https://juejin.im/equation?tex=%7Balpha_address%7D%3A8080%20-jar%20car-){saga_version}-exec.jar java -Dserver.port=8083 -Dalpha.cluster.address=alpha_address:8080 -Dcar.service.address=](https://juejin.im/equation?tex=%7Balpha_address%7D%3A8080%20-Dcar.service.address%3D){host_address}:8082 -Dhotel.service.address=host_address:8081 -jar booking-](https://juejin.im/equation?tex=%7Bhost_address%7D%3A8081%20%20-jar%20booking-){saga_version}-exec.jar[alpha_address不带http其他地址要带上http] ### 3.5 TCC解决方案项目地址https://github.com/QNJR-GROUP/EasyTransaction[对比tcc-transaction，Hmily，ByteTCC来说EasyTransaction性能最好，压测未发现错误], 当然你也可以使用上面提到的SAGA项目，也是支持TCC协议的。下面我们举个例子来看TCC是如何处理业务逻辑的。 eg：订单支付 1：订单服务-&gt;修改订单状态 2：库存服务-&gt;扣减库存 3：积分服务-&gt;增加积分 4：仓库服务-&gt;创建出库单 try阶段 1：订单服务-&gt;状态变更为“UpDating” 2：库存服务-&gt;可用库存减少1，冻结库存增加1 3：积分服务-&gt;积分不变，增加预备积分 4：仓库服务-&gt;创建出库单，状态设置为“UnKnown” confirm阶段 1：订单服务-&gt;状态变更为“已支付” 2：库存服务-&gt;冻结库存清零 3：积分服务-&gt;积分增加，预备积分清零 4：仓库服务-&gt;状态设置为“出库单已创建” cancel阶段 1：订单服务-&gt;状态变更为“已取消” 2：库存服务-&gt;可用库存增加，冻结库存清零 3：积分服务-&gt;预备积分清零 4：仓库服务-&gt;状态设置为“已取消” 4.小结 基本概念 优点 缺点 本地事务。事务由资源管理器（如DBMS）本地管理 严格的ACID 不具备分布事务处理能力 全局事务（DTP模型）TX协议：应用或应用服务器与事务管理器的接口 XA协议：全局事务管理器与资源管理器的接口 严格的ACID 效率非常低 JTA:面向应用、应用服务器与资源管理器的高层事务接口JTS:JTA事务管理器的实现标准，向上支持JTA，向下通过CORBA OTS实现跨事务域的互操作性EJB 简单一致的编程模型；跨域分布处理的ACID保证 DTP模型本身的局限；缺少充分公开的大规模、高可用、密集事务应用的成功案例 基于MQ 消息数据独立存储、独立伸缩；降低业务系统与消息系统间的耦合 一次消息发送需要两次请求；业务处理服务需实现消息状态回查接口 二阶段提交 原理简单，实现方便 同步阻塞：在二阶段提交的过程中，所有的节点都在等待其他节点的响应，无法进行其他操作。这种同步阻塞极大的限制了分布式系统的性能。 单点问题：协调者在整个二阶段提交过程中很重要，如果协调者在提交阶段出现问题，那么整个流程将无法运转。更重要的是，其他参与者将会处于一直锁定事务资源的状态中，而无法继续完成事务操作。 数据不一致：假设当协调者向所有的参与者发送commit请求之后，发生了局部网络异常，或者是协调者在尚未发送完所有 commit请求之前自身发生了崩溃，导致最终只有部分参与者收到了commit请求。这将导致严重的数据不一致问题。 容错性不好：如果在二阶段提交的提交询问阶段中，参与者出现故障，导致协调者始终无法获取到所有参与者的确认信息，这时协调者只能依靠其自身的超时机制，判断是否需要中断事务。显然，这种策略过于保守。换句话说，二阶段提交协议没有设计较为完善的容错机制，任意一个节点是失败都会导致整个事务的失败。 TCC 相对于二阶段提交，三阶段提交主要解决的单点故障问题，并减少了阻塞的时间。因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行 commit。而不会一直持有事务资源并处于阻塞状态 三阶段提交也会导致数据一致性问题。由于网络原因，协调者发送的 abort 响应没有及时被参与者接收到，那么参与者在等待超时之后执行了 commit 操作。这样就和其他接到 abort 命令并执行回滚的参与者之间存在数据不一致的情况。 SAGA 简单业务使用TCC需要修改原来业务逻辑，saga只需要添加一个补偿动作。由于没有预留动作所以不用担心资源释放的问题异常处理简单 由于没有预留动作导致补偿处理麻烦 业务各有各的不同，有些业务能容忍短期不一致，有些业务的操作可以幂等，无论什么样的分布式事务解决方案都有其优缺点，没有一个银弹能够适配所有。因此，业务需要什么样的解决方案，还需要结合自身的业务需求、业务特点、技术架构以及各解决方案的特性，综合分析，才能找到最适合的方案。]]></content>
      <tags>
        <tag>分布式事物</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis配置文件详解]]></title>
    <url>%2F2018%2F11%2F01%2FRedis%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[作者：纳木错来源：CSDN原文：https://blog.csdn.net/neubuffer/article/details/17003909版权声明：本文为博主原创文章，转载请附上博文链接！ 配置redis.conf文件： 1 daemonize no默认情况下，redis 不是在后台运行的，如果需要在后台运行，把该项的值更改为yes。 2 pidfile /var/run/redis.pid当Redis 在后台运行的时候，Redis 默认会把pid 文件放在/var/run/redis.pid，你可以配置到其他地址。当运行多个redis 服务时，需要指定不同的pid 文件和端口 3 port监听端口，默认为6379 4 #bind 127.0.0.1指定Redis 只接收来自于该IP 地址的请求，如果不进行设置，那么将处理所有请求，在生产环境中为了安全最好设置该项。默认注释掉，不开启 5 timeout 0设置客户端连接时的超时时间，单位为秒。当客户端在这段时间内没有发出任何指令，那么关闭该连接 6 tcp-keepalive 0指定TCP连接是否为长连接,”侦探”信号有server端维护。默认为0.表示禁用 7 loglevel noticelog 等级分为4 级，debug,verbose, notice, 和warning。生产环境下一般开启notice 8 logfile stdout配置log 文件地址，默认使用标准输出，即打印在命令行终端的窗口上，修改为日志文件目录 9 databases 16设置数据库的个数，可以使用SELECT 命令来切换数据库。默认使用的数据库是0号库。默认16个库 10save 900 1save 300 10save 60 10000 保存数据快照的频率，即将数据持久化到dump.rdb文件中的频度。用来描述”在多少秒期间至少多少个变更操作”触发snapshot数据保存动作 默认设置，意思是： if(在60 秒之内有10000 个keys 发生变化时){ 进行镜像备份 }else if(在300 秒之内有10 个keys 发生了变化){ 进行镜像备份 }else if(在900 秒之内有1 个keys 发生了变化){ 进行镜像备份 } 11 stop-writes-on-bgsave-error yes当持久化出现错误时，是否依然继续进行工作，是否终止所有的客户端write请求。默认设置”yes”表示终止，一旦snapshot数据保存故障，那么此server为只读服务。如果为”no”，那么此次snapshot将失败，但下一次snapshot不会受到影响，不过如果出现故障,数据只能恢复到”最近一个成功点” 12 rdbcompression yes在进行数据镜像备份时，是否启用rdb文件压缩手段，默认为yes。压缩可能需要额外的cpu开支，不过这能够有效的减小rdb文件的大，有利于存储/备份/传输/数据恢复 13 rdbchecksum yes读取和写入时候，会损失10%性能 14 rdbchecksum yes是否进行校验和，是否对rdb文件使用CRC64校验和,默认为”yes”，那么每个rdb文件内容的末尾都会追加CRC校验和，利于第三方校验工具检测文件完整性 15 dir ./数据库镜像备份的文件rdb/AOF文件放置的路径。这里的路径跟文件名要分开配置是因为Redis 在进行备份时，先会将当前数据库的状态写入到一个临时文件中，等备份完成时，再把该临时文件替换为上面所指定的文件，而这里的临时文件和上面所配置的备份文件都会放在这个指定的路径当中 16 # slaveof 设置该数据库为其他数据库的从数据库，并为其指定master信息。 17 masterauth当主数据库连接需要密码验证时，在这里指定 18 slave-serve-stale-data yes当主master服务器挂机或主从复制在进行时，是否依然可以允许客户访问可能过期的数据。在”yes”情况下,slave继续向客户端提供只读服务,有可能此时的数据已经过期；在”no”情况下，任何向此server发送的数据请求服务(包括客户端和此server的slave)都将被告知”error” 19 slave-read-only yesslave是否为”只读”，强烈建议为”yes” 20 # repl-ping-slave-period 10slave向指定的master发送ping消息的时间间隔(秒)，默认为10 21 # repl-timeout 60slave与master通讯中,最大空闲时间,默认60秒.超时将导致连接关闭 22 repl-disable-tcp-nodelay noslave与master的连接,是否禁用TCP nodelay选项。”yes”表示禁用,那么socket通讯中数据将会以packet方式发送(packet大小受到socket buffer限制)。 可以提高socket通讯的效率(tcp交互次数),但是小数据将会被buffer,不会被立即发送,对于接受者可能存在延迟。”no”表示开启tcp nodelay选项,任何数据都会被立即发送,及时性较好,但是效率较低，建议设为no 23 slave-priority 100适用Sentinel模块(unstable,M-S集群管理和监控),需要额外的配置文件支持。slave的权重值,默认100.当master失效后,Sentinel将会从slave列表中找到权重值最低(&gt;0)的slave,并提升为master。如果权重值为0,表示此slave为”观察者”,不参与master选举 24 # requirepass foobared设置客户端连接后进行任何其他指定前需要使用的密码。警告：因为redis 速度相当快，所以在一台比较好的服务器下，一个外部的用户可以在一秒钟进行150K 次的密码尝试，这意味着你需要指定非常非常强大的密码来防止暴力破解。 25 # rename-command CONFIG 3ed984507a5dcd722aeade310065ce5d (方式:MD5(‘CONFIG^!’))重命名指令,对于一些与”server”控制有关的指令,可能不希望远程客户端(非管理员用户)链接随意使用,那么就可以把这些指令重命名为”难以阅读”的其他字符串 26 # maxclients 10000限制同时连接的客户数量。当连接数超过这个值时，redis 将不再接收其他连接请求，客户端尝试连接时将收到error 信息。默认为10000，要考虑系统文件描述符限制，不宜过大，浪费文件描述符，具体多少根据具体情况而定 27 # maxmemory redis-cache所能使用的最大内存(bytes),默认为0,表示”无限制”,最终由OS物理内存大小决定(如果物理内存不足,有可能会使用swap)。此值尽量不要超过机器的物理内存尺寸,从性能和实施的角度考虑,可以为物理内存3/4。此配置需要和”maxmemory-policy”配合使用,当redis中内存数据达到maxmemory时,触发”清除策略”。在”内存不足”时,任何write操作(比如set,lpush等)都会触发”清除策略”的执行。在实际环境中,建议redis的所有物理机器的硬件配置保持一致(内存一致),同时确保master/slave中”maxmemory””policy”配置一致。 当内存满了的时候，如果还接收到set 命令，redis 将先尝试剔除设置过expire 信息的key，而不管该key 的过期时间还没有到达。在删除时， 将按照过期时间进行删除，最早将要被过期的key 将最先被删除。如果带有expire 信息的key 都删光了，内存还不够用，那么将返回错误。这样，redis 将不再接收写请求，只接收get 请求。maxmemory 的设置比较适合于把redis 当作于类似memcached的缓存来使用。 28 # maxmemory-policy volatile-lru内存不足”时,数据清除策略,默认为”volatile-lru”。 volatile-lru -&gt;对”过期集合”中的数据采取LRU(近期最少使用)算法.如果对key使用”expire”指令指定了过期时间,那么此key将会被添加到”过期集合”中。将已经过期/LRU的数据优先移除.如果”过期集合”中全部移除仍不能满足内存需求,将OOM.allkeys-lru -&gt;对所有的数据,采用LRU算法volatile-random -&gt;对”过期集合”中的数据采取”随即选取”算法,并移除选中的K-V,直到”内存足够”为止. 如果如果”过期集合”中全部移除全部移除仍不能满足,将OOMallkeys-random -&gt;对所有的数据,采取”随机选取”算法,并移除选中的K-V,直到”内存足够”为止volatile-ttl -&gt;对”过期集合”中的数据采取TTL算法(最小存活时间),移除即将过期的数据.noeviction -&gt;不做任何干扰操作,直接返回OOM异常另外，如果数据的过期不会对”应用系统”带来异常,且系统中write操作比较密集,建议采取”allkeys-lru” 29 # maxmemory-samples 3默认值3，上面LRU和最小TTL策略并非严谨的策略，而是大约估算的方式，因此可以选择取样值以便检查 30 # appendfilename appendonly.aofaof文件名字，默认为appendonly.aof 31appendfsync always appendfsync everysec appendfsync no 设置对appendonly.aof 文件进行同步的频率。always 表示每次有写操作都进行同步，everysec 表示对写操作进行累积，每秒同步一次。no不主动fsync，由OS自己来完成。这个需要根据实际业务场景进行配置 32 no-appendfsync-on-rewrite no在aof rewrite期间,是否对aof新记录的append暂缓使用文件同步策略,主要考虑磁盘IO开支和请求阻塞时间。默认为no,表示”不暂缓”,新的aof记录仍然会被立即同步 33 auto-aof-rewrite-percentage 100当Aof log增长超过指定比例时，重写log file， 设置为0表示不自动重写Aof 日志，重写是为了使aof体积保持最小，而确保保存最完整的数据。 34 auto-aof-rewrite-min-size 64mb触发aof rewrite的最小文件尺寸 35 lua-time-limit 5000lua脚本运行的最大时间 36 slowlog-log-slower-than 10000“慢操作日志”记录,单位:微秒(百万分之一秒,1000 * 1000),如果操作时间超过此值,将会把command信息”记录”起来.(内存,非文件)。其中”操作时间”不包括网络IO开支,只包括请求达到server后进行”内存实施”的时间.”0”表示记录全部操作 37 slowlog-max-len 128“慢操作日志”保留的最大条数,”记录”将会被队列化,如果超过了此长度,旧记录将会被移除。可以通过”SLOWLOG args”查看慢记录的信息(SLOWLOG get 10,SLOWLOG reset) 38 hash-max-ziplist-entries 512 hash类型的数据结构在编码上可以使用ziplist和hashtable。ziplist的特点就是文件存储(以及内存存储)所需的空间较小,在内容较小时,性能和hashtable几乎一样.因此redis对hash类型默认采取ziplist。如果hash中条目的条目个数或者value长度达到阀值,将会被重构为hashtable。 这个参数指的是ziplist中允许存储的最大条目个数，，默认为512，建议为128hash-max-ziplist-value 64 ziplist中允许条目value值最大字节数，默认为64，建议为1024 39list-max-ziplist-entries 512list-max-ziplist-value 64 对于list类型,将会采取ziplist,linkedlist两种编码类型。解释同上。 40 set-max-intset-entries 512intset中允许保存的最大条目个数,如果达到阀值,intset将会被重构为hashtable 41zset-max-ziplist-entries 128zset-max-ziplist-value 64 zset为有序集合,有2中编码类型:ziplist,skiplist。因为”排序”将会消耗额外的性能,当zset中数据较多时,将会被重构为skiplist。 42 activerehashing yes是否开启顶层数据结构的rehash功能,如果内存允许,请开启。rehash能够很大程度上提高K-V存取的效率 43client-output-buffer-limit normal 0 0 0client-output-buffer-limit slave 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60 客户端buffer控制。在客户端与server进行的交互中,每个连接都会与一个buffer关联,此buffer用来队列化等待被client接受的响应信息。如果client不能及时的消费响应信息,那么buffer将会被不断积压而给server带来内存压力.如果buffer中积压的数据达到阀值,将会导致连接被关闭,buffer被移除。 buffer控制类型包括:normal -&gt; 普通连接；slave -&gt;与slave之间的连接；pubsub -&gt;pub/sub类型连接，此类型的连接，往往会产生此种问题;因为pub端会密集的发布消息,但是sub端可能消费不足.指令格式:client-output-buffer-limit “,其中hard表示buffer最大值,一旦达到阀值将立即关闭连接;soft表示”容忍值”,它和seconds配合,如果buffer值超过soft且持续时间达到了seconds,也将立即关闭连接,如果超过了soft但是在seconds之后，buffer数据小于了soft,连接将会被保留.其中hard和soft都设置为0,则表示禁用buffer控制.通常hard值大于soft. 44 hz 10Redis server执行后台任务的频率,默认为10,此值越大表示redis对”间歇性task”的执行次数越频繁(次数/秒)。”间歇性task”包括”过期集合”检测、关闭”空闲超时”的连接等,此值必须大于0且小于500。此值过小就意味着更多的cpu周期消耗,后台task被轮询的次数更频繁。此值过大意味着”内存敏感”性较差。建议采用默认值。 45include /path/to/local.conf include /path/to/other.conf 额外载入配置文件。 46 dbfilename dump.rdb镜像备份文件的文件名，默认为 dump.rdb 47 appendonly no默认情况下，redis 会在后台异步的把数据库镜像备份到磁盘，但是该备份是非常耗时的，而且备份也不能很频繁。所以redis 提供了另外一种更加高效的数据库备份及灾难恢复方式。开启append only 模式之后，redis 会把所接收到的每一次写操作请求都追加到appendonly.aof 文件中，当redis 重新启动时，会从该文件恢复出之前的状态。但是这样会造成appendonly.aof 文件过大，所以redis 还支持了BGREWRITEAOF 指令，对appendonly.aof 进行重新整理。如果不经常进行数据迁移操作，推荐生产环境下的做法为关闭镜像，开启appendonly.aof，同时可以选择在访问较少的时间每天对appendonly.aof 进行重写一次。 另外，对master机器,主要负责写，建议使用AOF,对于slave,主要负责读，挑选出1-2台开启AOF，其余的建议关闭]]></content>
      <tags>
        <tag>redis</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ReentrantLock源码解析]]></title>
    <url>%2F2018%2F10%2F30%2FReentrantLock%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言 ReentrantLock，可重入锁，是一种递归无阻塞的同步方式，它比Synchronized更强大，可以减小死锁的概率。 ReentrantLock提供了公平锁和非公平锁的选择，默认是非公平锁。当调用构造方法时传参为true时为公平锁，否则为非公平锁。 123public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync(); &#125; 公平锁与非公平锁的区别在于公平锁的获取是有顺序的，释放锁是不分公平锁还是非公平锁的。公平锁的效率和吞吐量相对于非公平锁较低。 ReentrantLock源码解析1.构造方法 ReentrantLock类有两个构造方法，一个是无参构造，默认是非公平锁，另外一种是有参构造，前言中已有介绍。 123456public ReentrantLock() &#123; sync = new NonfairSync();&#125;public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 2.锁的获取(以非公平锁为例)123public void lock() &#123; sync.lock();&#125; Sync是ReentrantLock的一个内部类，它继承了AQS(AbstractQueuedSynchronizer)，它分为两个子类,NonfairSync(非公平锁)和FairSync(公平锁)。 因为ReentrantLock里的大部分方法都是委托给Sync实现的，而Sync有两个子类，所以直接调用的是非公平锁的lock方法。 123456final void lock() &#123; if (compareAndSetState(0, 1))//CAS setExclusiveOwnerThread(Thread.currentThread());//设置独占线程 else acquire(1);&#125; 首先先做一个CAS，比较当前的状态和期望的状态是否一致，不一致更新，然后尝试去获取锁，如果获取锁成功，则设置当前线程为独占线程。 如果获取所失败，则调用acquire(1)方法 1234567AbstractQueuedSynchronizer类里的方法public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 首先先调用tryAcquire(arg)方法 12345678910111213141516171819202122232425262728protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires);&#125;final boolean nonfairTryAcquire(int acquires) &#123; //当前线程 final Thread current = Thread.currentThread(); //获取同步状态 int c = getState(); //state == 0,表示没有该锁处于空闲状态 if (c == 0) &#123; //获取锁成功，设置为当前线程所有 if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; //线程重入 //判断锁持有的线程是否为当前线程 else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false;&#125; 先获取当前线程及其状态， 如果当前线程的状态等于0，说明Lock不被任何线程所占有，则获取锁成功，然后CAS，设置独占线程，返回true 如果当前线程状态不等于0，就判断锁所独占的线程是否为当前线程 如果为当前线程，就更改当前线程状态为锁占有(1)，返回true 否则，返回false acquireQueued()方法就是不停循环的方式获取线程的结果 12345if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg))&#123; selfInterrupt();&#125;如果没获取到队列的状态并且不停循环的方式获取线程的结果中断失败标志为true 3.锁的释放123public void unlock() &#123; sync.release(1);&#125; 123456789public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; 123456789101112131415protected final boolean tryRelease(int releases) &#123; //减掉releases int c = getState() - releases; //如果释放的不是持有锁的线程，抛出异常 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; //state == 0 表示已经释放完全了，其他线程可以获取同步状态了 if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free; &#125;]]></content>
      <tags>
        <tag>锁</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GithubPages+Yilia搭建个人博客]]></title>
    <url>%2F2018%2F10%2F25%2FGithubPages-Yilia%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[前言：作为一个程序员，日常要学很多东西，如果不及时记录一下，之后就会忘记，而且搭建一个自己的个人博客网站也能在简历上显露一下，提高面试机会。于是乎，我就想搭建一个自己的博客，本来是想着自己写，包括数据库，前端和后台的编码，但是无奈小弟技术有限，时间有限，精力有限，所以就果断放弃。看到网上大神们都用的是githubPages+hexo搭建自己的博客，所以就依葫芦画瓢搞了一个。在这里mark一下自己建站的过程，说多了全是泪啊。 1.准备工作 因为是使用的github pages，所以你要有一个github账号 安装nodejs，npm（网上全是资料） 安装hexo,，网上很多教程。 本机安装git 因为博客是用markdown编写的，所以掌握markdown的语法。 准备自己的域名（当然如果你不打算用自己的域名，忽略此步） 2.搭建博客2.1 搭建仓库 在github上搭建一个仓库，名称必须为你的github用户名.github.io，比如我的github用户名为zhjs0523，所以仓库名称为zhjs0523.githu.io。这样你的博客的访问地址就是：http://www.你的github用户名.github.io，当然如果你解析了域名，也可以用域名访问。 2.2 配置博客 安装完hexo后，初始化hexo，然后会生成如下图的目录文件 然后打开博客目录下的_config.yml文件进行配置，具体配置可参考我的，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# Hexo Configuration## Docs: http://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: ### 个人博客的titlesubtitle: ### 子标题 可以是一句话description: ### 描述author: ### 作者language:timezone:# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://yoursite.comroot: /permalink: :year/:month/:day/:title/permalink_defaults:# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace:# Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions## Plugins: http://hexo.io/plugins/## Themes: http://hexo.io/themes/theme: yilia ###主题 hexo默认是landscape,可以去hexo 主题下自己下载jsonContent: meta: false pages: false posts: title: true date: true path: true text: false raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true# Deployment## Docs: http://hexo.io/docs/deployment.htmldeploy: type: git repository: https://github.com/zhjs0523/zhjs0523.github.io.git ##个人仓库的地址，就是上述你所建的仓库的地址 branch: master 注意：hexo的配置文件中，所有的配置参数key与value之间都有一个空格。 2.3 配置主题我用的主题是yilia，在/theme/目录下会有yilia文件夹，/theme/yilia里有个_config.yml配置文件: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125# Headermenu: 主页: / 归档: /archives# SubNavsubnav: github: ### weibo: ### #rss: "#" #zhihu: "#" #qq: "#" weixin: "/img/wechat.jpeg" #jianshu: "/information/information.md" #douban: "#" #segmentfault: "#" #bilibili: "#" #acfun: "#" #mail: "mailto:litten225@qq.com" #facebook: "#" #google: "#" #twitter: "#" #linkedin: "#"rss: /atom.xml# 是否需要修改 root 路径# 如果您的网站存放在子目录中，例如 http://yoursite.com/blog，# 请将您的 url 设为 http://yoursite.com/blog 并把 root 设为 /blog/。root: /# 文章太长，截断按钮文字excerpt_link: more# 文章卡片右下角常驻链接，不需要请设置为falseshow_all_link: '展开全文'# 数学公式mathjax: false# 是否在新窗口打开链接open_in_new: false# 打赏# 打赏type设定：0-关闭打赏； 1-文章对应的md文件里有reward:true属性，才有打赏； 2-所有文章均有打赏reward_type: 2# 打赏wordingreward_wording: '赏个好友位呗!!!'# 支付宝二维码图片地址，跟你设置头像的方式一样。比如：/assets/img/alipay.jpgalipay:# 微信二维码图片地址weixin: /img/wechat.jpeg# 目录# 目录设定：0-不显示目录； 1-文章对应的md文件里有toc:true属性，才有目录； 2-所有文章均显示目录toc: 1# 根据自己的习惯来设置，如果你的目录标题习惯有标号，置为true即可隐藏hexo重复的序号；否则置为falsetoc_hide_index: true# 目录为空时的提示toc_empty_wording: '目录，不存在的…'# 是否有快速回到顶部的按钮top: true# Miscellaneousbaidu_analytics: ''google_analytics: ''favicon: /favicon.png#你的头像urlavatar: /img/head.jpeg#是否开启分享share_jia: true#评论：1、多说；2、网易云跟帖；3、畅言；4、Disqus；5、Gitment#不需要使用某项，直接设置值为false，或注释掉#具体请参考wiki：https://github.com/litten/hexo-theme-yilia/wiki/#1、多说duoshuo: false#2、网易云跟帖wangyiyun: false#3、畅言changyan_appid: falsechangyan_conf: false#4、Disqus 在hexo根目录的config里也有disqus_shortname字段，优先使用yilia的disqus: false#5、Gitmentgitment_owner: 'zhjs0523' #你的 GitHub IDgitment_repo: 'zhjs0523.github.io' #存储评论的 repogitment_oauth: client_id: '' #client ID client_secret: '' #client secret# 样式定制 - 一般不需要修改，除非有很强的定制欲望…style: # 头像上面的背景颜色 header: '#4d4d4d' # 右滑板块背景 slider: 'linear-gradient(200deg,#a0cfe4,#e8c37e)'# slider的设置slider: # 是否默认展开tags板块 showTags: true# 智能菜单# 如不需要，将该对应项置为false# 比如#smart_menu:# friends: falsesmart_menu: innerArchive: '所有文章' friends: '友链' aboutme: '关于我'friends: 我的CSDN博客: https://blog.csdn.net/u010368749 tianzhisheng的博客: http://www.54tianzhisheng.cn/ 程序猿DD的博客: http://blog.didispace.com/aboutme: 码农一枚&lt;br&gt;&lt;br&gt;不只技术，还有生活 2.4 新建&amp;上传博客hexo new ‘My First blog’，执行该命令会在source/_post/下创建一个My First blog.md,编写该md文件。 hexo clean 清理一下博客 hexo generate(简写为hexo g) 生成博客 会生成一些HTML，js和css文件 hexo s 在本地启动博客服务 访问地址：http://localhost:4000即可看到自己的博客 hexo deploy(hexo d) 上传博客到github上，你可以直接访问 http://www.你的github用户名.github.io直接访问你的博客。可能会需要等待10分钟左右才能看到效果。 3.绑定域名毕竟是搭建一个自己的博客，还是用自己的域名才会显得更加狂拽酷炫吊炸天，所以申请一个个人域名，我是在阿里云上申请的。因为工商部现在对域名的管理非常严格，所以如果你的服务器是大陆的，是需要在工商部备案的，当然如果是国外的服务器，是无需备案的。 3.1 域名解析申请完域名之后，就要做域名解析啦，这样你才能通过域名访问你的博客，解析也比较简单，在阿里云的https://account.aliyun.com/login/login.htm 后台中进行解析 3.2 配置github pages的custom domain 注意： 配置完之后，每次提交（hexo deploy）的时候,上诉的custom domain会把值清空，所以你应该在source/_posts/目录下建一个CNAME的文件，文件内容就写你自己的域名。 4.其他配置4.1 评论 常见的评论系统有畅言，友言，多说，网易云跟帖，Gitment等。 畅言是比较常用的，但是畅言是需要备案的。 友言和多说已经停止服务啦。 gitment是一个不错的评论系统，直接使用github的账号登录评论，而且无需编写后端代码，配置一下即可。 4.1.1 注册 OAuth Application通过https://github.com/settings/applications/new配置OAuth，homePage URL 和Application callback URL都填写你自己的域名或者你的github用户名.github.io，其他的随便填写，提交完之后你会得到一个client ID和client secret. 4.1.2 配置在/theme/yilia/的_config.yml 文件中，找到gitment 的配置，配置一下gitment参数 4.1.3 初始化评论配置完成之后， 如果你的界面如果如下所示，那说明你配置成功。 如果你的界面如下所示，那就有可能是多种原因造成，我当时配置时出现了这种情况，在网上找了很多种解决方案，都没有成功，最后发现是因为配置的gitment的URL错误导致的。 打开你的blog本地仓库themes\yilia\layout\_partial\post\gitment.ejs将 12&lt;link rel="stylesheet" href="//imsun.github.io/gitment/style/default.css"&gt;&lt;script src="//imsun.github.io/gitment/dist/gitment.browser.js"&gt;&lt;/script&gt; 更改为 12&lt;link rel="stylesheet" href="https://jjeejj.github.io/css/gitment.css"&gt;&lt;script src="https://jjeejj.github.io/js/gitment.js"&gt;&lt;/script&gt; 然后再登录你的github账号初始化即可。 其他原因传送门: https://www.jianshu.com/p/57afa4844aaa https://github.com/imsun/gitment/issues/95 https://github.com/imsun/gitment/issues/81 也有可能会出现的Error：validation failed,这是因为gitment默认的id是页面id，就是文章的唯一的标识，而限制的长度是50，默认的是以URL为id的，所以需要控制在50位之内，所以以创建博客的时间做id，修改如下： 修改文件themes/yilia/layout/_partial/post/gitment.ejs 原来是 1id: &quot;&lt;%=url%&gt;&quot;, 修改为： 1id: &quot;&lt;%=page.date%&gt;&quot;,]]></content>
      <tags>
        <tag>githubPages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池最优大小的计算]]></title>
    <url>%2F2018%2F10%2F24%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[前言​ 在高并发的环境下，多线程的使用变得越来越重要，但是在很多程序员眼中，不太关注于设置线程池的大小。然而，如果线程池过大，那么大量的线程将在相对很少的CPU和内存资源上发生竞争，这不仅会导致更高的内存使用量，而且还可能耗尽资源；如果线程池设置的过小，那么将导致许多空闲的处理器无法执行工作，从而降低吞吐率。 计算公式线程池最优大小=CPU个数 CPU期望利用率 (1 + 任务等待时间 / 任务处理时间) 假设有一个应用，部署在8核服务器上，期望任务执行CPU占比约20%,任务平均等待时间为150ms,r任务平均处理时间为50ms线程池最优大小=80.2（1+150/50）=3]]></content>
      <tags>
        <tag>并发</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Resource和Autowired的区别]]></title>
    <url>%2F2018%2F10%2F17%2FResource%E5%92%8CAutowired%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[之前面试的时候遇到过这么一个面试题，当时太嫩，只知道使用，不知道区别，所以被狠狠的问住啦，因此在这儿记录一下。 相同点 两者都是关于bean的注入 都能写到字段上或者写在set方法上 不同点 类别 @Resource @Autowired 所属包 J2EE的注解 javax.annotation.Resource Spring的注解 org.springframework.beans.factory.annotation.Autowired 装配类型 按名称装配 按类型装配 1大白话就是：如果一个接口有两个实现类，用@Autowired就不知道注入哪一个实现类（可以再加上@Qualifier（“bean名称”）注解），而@Resource是有name属性的(@Resource(name = &quot;bean名称&quot;))。]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[synchronized和Lock的区别]]></title>
    <url>%2F2018%2F10%2F12%2Fsynchronized%E5%92%8CLock%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言： 只要你是Java程序员，你就一定离不开多线程，而多线程也是面试过程中面试官经常询问的问题，而多线程最常见的就是synchronized和Lock，小编以前在面试的过程中，多次被问到这个问题，故而在这里整理一下。 区别 类别 synchronized lock 存在层次 java关键字，jvm层次 一个类 锁的释放 1.获取锁的线程执行完同步代码，释放锁；2.发生异常时，jvm自动释放锁 必须手动释放，可以定义在finally代码块中 锁的获取 A线程获得锁，B线程等待；A线程阻塞，B线程一直等待 见下文 锁的状态 无法判断 可以判断 锁类型 可重入，轻量锁，偏向锁，不可中断，非公平 可重入，可中断，公平和非公平皆可 性能 少量同步 大量同步 Lock123456789101112131415161718192021222324252627282930package java.util.concurrent.locks;import java.util.concurrent.TimeUnit;public interface Lock &#123; /** * 获取锁，如果锁被暂用则一直等待知道锁释放 */ void lock(); /** * 如果线程A在获取锁的时候进入了等待队列，则可以中断线程A,去做其他的事情 */ void lockInterruptibly() throws InterruptedException; /** * 试图获取锁，如果被暂用返回false，否则返回true */ boolean tryLock(); /** * 试图获取锁，如果锁被暂用，则等待time时间，到期则中断该线程，抛出InterruptedException异常 */ boolean tryLock(long time, TimeUnit unit) throws InterruptedException; /** * 释放锁 */ void unlock(); /** * 返回一个ConditionObject对象 */ Condition newCondition();&#125;]]></content>
      <tags>
        <tag>锁</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序员你为什么这么累:编码习惯之工具类规范]]></title>
    <url>%2F2018%2F09%2F29%2F%E7%A8%8B%E5%BA%8F%E5%91%98%E4%BD%A0%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E7%B4%AF-%E7%BC%96%E7%A0%81%E4%B9%A0%E6%83%AF%E4%B9%8B%E5%B7%A5%E5%85%B7%E7%B1%BB%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[本文作者：晓风轻原文链接：https://zhuanlan.zhihu.com/p/29199049版权归作者所有，转载请注明出处 一个项目不可能没有工具类，工具类的初衷是良好的，代码重用，但到了后面工具类越来越乱，有些项目工具类有几十个，看的眼花缭乱，还有不少重复。如何编写出好的工具类，我有几点建议： 隐藏实现就是要定义自己的工具类，尽量不要在业务代码里面直接调用第三方的工具类。这也是解耦的一种体现。如果我们不定义自己的工具类而是直接使用第三方的工具类有2个不好的地方： 不同的人会使用不同的第三方工具库，会比较乱。 将来万一要修改工具类的实现逻辑会很痛苦。 以最简单的字符串判空为例，很多工具库都有 StringUtils工具类，如果我们使用commons的工具类，一开始我们直接使用 StringUtils.isEmpty ，字符串为空或者空串的时候会返回为true，后面业务改动，需要改成如果全部是空格的时候也会返回true，怎么办？我们可以改成使用 StringUtils.isBlank 。看上去很简单，对吧？ 如果你有几十个文件都调用了，那我们要改几十个文件，是不是有点恶心？再后面发现，不只是英文空格，如果是全角的空格，也要返回为true，怎么办？StringUtils上的方法已经不能满足我们的需求了，真不好改了。。。 所以我的建议是，一开始就自己定义一个自己项目的StringUtil，里面如果不想自己写实现，可以直接调用commons的方法，如下： 123public static boolean isEmpty(String str) &#123; return org.apache.commons.lang3.StringUtils.isEmpty(str);&#125; 后面全部空格也返回true的时候，我们只需要把isEmpty改成isBlank；再后面全部全角空格的时候也返回true的话，我们增加自己的逻辑即可。我们只需要改动和测试一个地方。 在举一个真实一点的例子，如复制对象的属性方法。 一开始，如果我们自己不定义工具类方法，那么我们可以使用 org.springframework.beans.BeanUtils.copyProperties(source, dest)这个工具类来实现，就一行代码，和调用自己的工具类没有什么区别。看上去很OK，对吧？ 随着业务发展，我们发现这个方式的性能或者某些特性不符合我们要求，我们需要修改改成 commons-beanutils包里面的方法，org.apache.commons.beanutils.BeanUtils.copyProperties(dest, source)，这个时候问题来了，第一个问题，它的方法的参数顺序和之前spring的工具类是相反的，改起来非常容易出错！第二个问题，这个方法有异常抛出，必须声明，这个改起来可要命了！结果你发现，一个看上去很小的改动，改了几十个文件，每个改动还得测试一次，风险不是那么得小。有一点小奔溃了，是不是？ 等你改完之后测试完了，突然有一天需要改成，复制参数的时候，有些特殊字段需要保留（如对象id）或者需要过滤掉（如密码）不复制，怎么办？这个时候我估计你要崩溃了吧？不要觉得我是凭空想象，编程活久见，你总会遇到的一天！ 所以，我们需要定义自己的工具类函数，一开始我定义成这样子。 123public void copyAttribute(Object source, Object dest) &#123; org.springframework.beans.BeanUtils.copyProperties(source, dest);&#125; 后面需要修改为commons-beanutis的时候，我们改成这样即可，把参数顺序掉过来，然后处理了一下异常，我使用的是Lombok的SneakyThrows来处理异常，你也可以捕获掉抛出运行时异常，个人喜好。 1234@SneakyThrowspublic void copyAttribute(Object source, Object dest) &#123; org.apache.commons.beanutils.BeanUtils.copyProperties(dest, source);&#125; 再后面，复制属性的时候需要保留某些字段或者过滤掉某些字段，我们自己参考其他库实现一次即可，只改动价格和测试一个文件一个方法，风险非常可控。 还记得我之前的帖子里说的需求变更吗？你可以认为这算需求变更，但同样的需求变更，我一个小时改完测试，没有任何风险轻轻松松上线，你可能满头大汗加班加点还担心出问题。。。 使用父类/接口上面那点隐藏实现，说到底是封装/解耦的思想，而现在说的这点是抽象的思想，做好了这点，我们就能编写出看上去很专业的工具类。这点很好理解，但是我们容易忽略。 举例，假设我们写了一个判断arraylist是否为空的函数，一开始是这样的。 123public static boolean isEmpty(ArrayList&lt;?&gt; list) &#123; return list == null || list.size() == 0;&#125; 这个时候，我们需要思考一下参数的类型能不能使用父类。我们看到我们只用了size方法，我们可以知道size方法再list接口上有，于是我们修改成这样。 123public static boolean isEmpty(List&lt;?&gt; list) &#123; return list == null || list.size() == 0;&#125; 后面发现，size方法再list的父类/接口Collection上也有，那么我们可以修改为最终这样。 123public static boolean isEmpty(Collection&lt;?&gt; list) &#123; return list == null || list.size() == 0;&#125; 到了这部，Collection没有父类/接口有size方法了，修改就结束了。最后我们需要把参数名字改一下，不要再使用list。改完后，所有实现了Collection都对象都可以用，最终版本如下： 123public static boolean isEmpty(Collection&lt;?&gt; collection) &#123; return collection == null || collection.size() == 0;&#125; 是不是看上去通用多了 ，看上去也专业多了？上面的string相关的工具类方法，使用相同的思路，我们最终修改一下，把参数类类型由String修改为CharSequence，参数名str修改为cs。如下： 123public static boolean isEmpty(CharSequence cs) &#123; return org.apache.commons.lang3.StringUtils.isEmpty(cs);&#125; 思路和方法很简单，但效果很好，写出来的工具类也显得很专业！总结一下，思路是抽象的思想，主要是修改参数类型，方法就是往上找父类/接口，一直找到顶为止，记得修改参数名。 使用重载编写衍生函数组开发过的兄弟都知道，有一些工具库，有一堆的重载函数，调用起来非常方便，经常能直接调用，不需要做参数转换。这些是怎么样编写出来的呢？我们举例说明。 现在需要编写一个方法，输入是一个utf-8格式的文件的文件名，把里面内容输出到一个list。我们刚刚开始编写的时候，是这个样子的 1234567891011121314public static List&lt;String&gt; readFile2List(String filename) throws IOException &#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); File file = new File(filename); FileInputStream fileInputStream = new FileInputStream(file); BufferedReader br = new BufferedReader(new InputStreamReader(fileInputStream, "UTF-8")); // XXX操作 return list;&#125; 我们先实现，实现完之后我们做第一个修改，很明显，utf-8格式是很可能要改的，所以我们先把它做为参数提取出去，方法一拆为二，就变成这样。 123456789101112131415161718public static List&lt;String&gt; readFile2List(String filename) throws IOException &#123; return readFile2List(filename, "UTF-8");&#125;public static List&lt;String&gt; readFile2List(String filename, String charset) throws IOException &#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); File file = new File(filename); FileInputStream fileInputStream = new FileInputStream(file); BufferedReader br = new BufferedReader(new InputStreamReader(fileInputStream, charset)); // XXX操作 return list;&#125; 多了一个方法，直接调用之前的方法主体，主要的代码还是只有一份，之前的调用地方不需要做任何修改！可以放心修改。 然后我们在看里面的实现，下面这2行代码里面，String类型的filename会变化为File类型，然后在变化为FileInputStream 类型之后才使用。 File file = new File(filename);FileInputStream fileInputStream = new FileInputStream(file); 这里我们就应该想到，用户可能直接传如File类型，也可能直接传入FileInputStream类型，我们应该都需要支持，而不需要用户自己做类型的处理！在结合上一点的使用父类，把FileInputStream改成父类InputStream，我们最终的方法组如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package plm.common.utils;import java.io.BufferedReader;import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStream;import java.io.InputStreamReader;import java.util.ArrayList;import java.util.List;import org.apache.commons.io.IOUtils;/** * 工具类编写范例，使用重载编写不同参数类型的函数组 * * @author 晓风轻 https://github.com/xwjie/PLMCodeTemplate * */public class FileUtil &#123; private static final String DEFAULT_CHARSET = "UTF-8"; public static List&lt;String&gt; readFile2List(String filename) throws IOException &#123; return readFile2List(filename, DEFAULT_CHARSET); &#125; public static List&lt;String&gt; readFile2List(String filename, String charset) throws IOException &#123; FileInputStream fileInputStream = new FileInputStream(filename); return readFile2List(fileInputStream, charset); &#125; public static List&lt;String&gt; readFile2List(File file) throws IOException &#123; return readFile2List(file, DEFAULT_CHARSET); &#125; public static List&lt;String&gt; readFile2List(File file, String charset) throws IOException &#123; FileInputStream fileInputStream = new FileInputStream(file); return readFile2List(fileInputStream, charset); &#125; public static List&lt;String&gt; readFile2List(InputStream fileInputStream) throws IOException &#123; return readFile2List(fileInputStream, DEFAULT_CHARSET); &#125; public static List&lt;String&gt; readFile2List(InputStream inputStream, String charset) throws IOException &#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); BufferedReader br = null; try &#123; br = new BufferedReader(new InputStreamReader(inputStream, charset)); String s = null; while ((s = br.readLine()) != null) &#123; list.add(s); &#125; &#125; finally &#123; IOUtils.closeQuietly(br); &#125; return list; &#125;&#125; 怎么样？6个方法，实际上代码主体只有一份，但提供各种类型的入参，调用起来很方便。开发组长编写的时候，多费一点点时间，就能写来看上去很专业调用起来很方便的代码。如果开发组长不写好，开发人员发现现有的方法只能传String，她要传的是InputStream，她又不敢改原来的代码，就会copy一份然后修改一下，就多了一份重复代码。代码就是这样烂下去了。。。 关键点，多想一步，根据参数变化编写各种类型的入参函数，需要保证函数主要代码只有一份。 使用静态引入工具类的一个问题就是容易泛滥，主要原因是开发人员找不到自己要用的方法，就自己写一个，开发人员很难记住类名，你也不可能天天代码评审。 所以要让开发人员容易找到，我们可以使用静态引入，在Eclipse里面这样导入： 这样，任何地方开发人员只要一敲就可以出来，然后再约定一下项目组方法名规范，这样工具类的使用就会简单很多！ 物理上独立存放这点是我的习惯，我习惯把和业务无关的代码放到独立的工程或者目录，在物理上要分开，专人维护。不是所有人都有能力写工具类，独立存放专门维护，专门的权限控制有助于保证代码的纯洁和质量。这样普通的开发人员就不会随意修改。 例如我的范例工程里面，专门建立了一个source目录存放框架代码，工具类也在里面，这里的代码，只有我一个人会去修改： 总结几乎所有人都知道面向对象的思想有抽象封装，但几个人真正能做到，其实有心的话，处处都能体现出这些思想。编写工具类的时候需要注意参数的优化，而且大型项目里面不要在业务代码里面直接调用第三方的工具类，然后就是多想一步多走一步，考虑各种类型的入参，这样你也能编写出专业灵活的工具类！ Github：晓风轻的代码模板：https://github.com/xwjie/PLMCodeTemplate ，欢迎加星fork]]></content>
      <tags>
        <tag>编程规范</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单可重入锁的实例]]></title>
    <url>%2F2018%2F09%2F27%2F%E7%AE%80%E5%8D%95%E5%8F%AF%E9%87%8D%E5%85%A5%E9%94%81%E7%9A%84%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[一.可重入锁的概念当对一个对象加锁时，同一个线程再次可以再次进入，无需阻塞等待。 二.实现1.哪些是可重入锁ReentrantLock和synchronized都是可重入锁 2.ReentrantLock和Synchronized的异同1.相同点 都是可重入锁，两者都是同一个线程每进入一次，计数器加1，等到计数器变为0时才释放锁。都是在用户态把加锁问题解决，避免进入内核造成阻塞，在synchronized引入轻量锁，偏向锁之后，两者性能差不多，官方推荐使用synchronized。 2)不同点 ReentrantLock是jdk实现的，synchronized是jvm实现的；ReentrantLock需要手动释放锁，synchronized是由编译器自动释放；ReentrantLock可以指定是公平锁还是非公平锁，synchronized只能是非公平锁。 3.代码实现123456789101112131415161718192021public static void main(String[] args)&#123; ReenTranlock r = new ReenTranlock(); new Thread(()-&gt;&#123; r.func1(); &#125;).start(); &#125; public synchronized void func1()&#123; System.out.println("func1 executing..."); func2(); &#125; public synchronized void func2()&#123; System.out.println("func2 executing..."); &#125;输出结果：func1 executing...func2 executing... 在方法上加锁，即为对象加锁，当线程调用func1方法时，func1方法中调用了func2方法，而此时并未释放锁，线程可以重新进入获得锁。 注意：多线程的测试不能在Junit4中进行，因为Junit4没有等待子线程完成再关闭主线程。]]></content>
      <tags>
        <tag>多线程</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[left join加上where条件的困惑]]></title>
    <url>%2F2018%2F09%2F18%2Fleft-join%E5%8A%A0%E4%B8%8Awhere%E6%9D%A1%E4%BB%B6%E7%9A%84%E5%9B%B0%E6%83%91%2F</url>
    <content type="text"><![CDATA[left join的困惑：一旦加上where条件，则显示的结果等于inner join将where 换成 and 用where 是先连接然后再筛选用and 是先筛选再连接 数据库在通过连接两张或多张表来返回记录时，都会生成一张中间的临时表，然后再将这张临时表返回给用户。 在使用left jion时，on和where条件的区别如下： 1、 on条件是在生成临时表时使用的条件，它不管on中的条件是否为真，都会返回左边表中的记录。 2、where条件是在临时表生成好后，再对临时表进行过滤的条件。这时已经没有left join的含义（必须返回左边表的记录）了，条件不为真的就全部过滤掉 假设有两张表: Tab1: id size 1 10 2 20 3 30 Tab2: size name 10 AAA 20 BBB 20 CCC 现有以下两条SQL: 121、select * form tab1 left join tab2 on tab1.size = tab2.size where tab2.name=’AAA’2、select * form tab1 left join tab2 on tab1.size = tab2.size and tab2.name=’AAA’ 第一条SQL的执行过程是: 先根据on 条件tab1.size = tab2.size生成一张中间表: tab1.id tab1.size tab2.size tab2.name 1 10 10 AAA 2 20 20 BBB 2 20 20 CCC 3 30 (null) (null) 然后再从中间表中筛选where条件tab2.name=’AAA’: tab1.id tab1.size tab2.size tab2.name 1 10 10 AAA 第二条SQL的执行过程是: 直接根据on条件tab1.size = tab2.size and tab2.name=’AAA’查询出来结果 tab1.id tab1.size tab2.size tab2.name 1 10 10 AAA 2 20 null Null 3 30 Null Null 两条语句执行的结果不同。 总结：产生以上结果的原因就是SQL语句的执行计划的顺序不同（explain），还有就是left join,right join,full join的特殊性，不管on上的条件是否为真都会返回left或right表中的记录，full则具有left和right的特性的并集。 而inner jion没这个特殊性，则条件放在on中和where中，返回的结果集是相同的。]]></content>
      <tags>
        <tag>DB</tag>
      </tags>
  </entry>
</search>
